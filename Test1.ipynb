{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:01.437590Z",
     "start_time": "2021-03-08T08:57:01.433587Z"
    },
    "id": "8BEr31BsugTD"
   },
   "outputs": [],
   "source": [
    "## Call pip install on 1st time only\n",
    "# %%capture cap\n",
    "# pip install geneticalgorithm\n",
    "# pip install import-ipynb\n",
    "# pip install git+https://github.com/hyperopt/hyperopt-sklearn.git\n",
    "# pip install platypus-opt\n",
    "# pip install numpy=1.19.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qnINu0VTt8a"
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:32.256312Z",
     "start_time": "2021-03-08T08:57:01.444594Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gmaL2jdDguo",
    "outputId": "8c68f3c6-99a4-4193-bccf-064ad3b1c3c8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import variation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest,SelectFromModel,RFE,chi2,mutual_info_classif,f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,auc,roc_curve\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# import statistics\n",
    "\n",
    "# import import_ipynb\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing, extra_trees,random_forest\n",
    "from hyperopt import tpe\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H17noAJObOO7"
   },
   "source": [
    "# **Parameter Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T10:06:17.869249Z",
     "start_time": "2021-03-13T10:06:17.797624Z"
    },
    "id": "vyOBJru1a2MY"
   },
   "outputs": [],
   "source": [
    "###--------------------------General--------------------------   \n",
    "\n",
    "data_folder=\"./data_myron/\"\n",
    "ensembl_and_gene_names = \"./R/data/Homo_sapiens.GRCh19.longest.GSE89843.ready.gtf\"\n",
    "data_path = data_folder + \"GSE89843_TEP_Count_Matrix_annot_ready.txt\"\n",
    "labels_path = data_folder + \"GSE89843_patients_characteristics_formatted_full.csv\"\n",
    "            \n",
    "n_folds=5\n",
    "clf = LogisticRegression( solver='sag', multi_class='multinomial',random_state=42)  \n",
    "shuffle_random_seed = 20\n",
    "verbose=True\n",
    "##--------------------------normalization--------------------------\n",
    "normalization_selected = \"TPM\" # \"TPM\" \"TMM\"  \"ruvseq_diff\"    \"ruvseq_cv\"\n",
    "normalisation_method_chosen=normalization_selected\n",
    "forced=True  # this parameter is to do normalization from scrach or reading those ones done befor\n",
    "top_f=50\n",
    "##--------------------------KBest--------------------------\n",
    "mode_KBest ='iterative'  #or fixed  iterative\n",
    "# mode_KBest ='fixed'  #or fixed  iterative\n",
    "\n",
    "estimator_KBest_Iterative = LogisticRegression( solver='sag', multi_class='multinomial',random_state=42)\n",
    "metric_of_KBest = mutual_info_classif   #chi2,mutual_info_classif,f_classif\n",
    "k_KBest=1000\n",
    "##--------------------------SelectFromModel--------------------------\n",
    "estimator_SelectFromModel=RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "##--------------------------REF--------------------------\n",
    "step_REF=1\n",
    "estimator_RFE=RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "n_final_features_to_select=5\n",
    "##--------------------------EVOFS--------------------------\n",
    "estimator_EVOFS=LogisticRegression(solver='saga', multi_class='multinomial',random_state=42,penalty='l1')\n",
    "score_func=mutual_info_classif   # chi2,mutual_info_classif,f_classif , it is recommended to set same as metric_of_KBest\n",
    "min_features=2\n",
    "max_generations=1000\n",
    "##--------------------------skmoefs--------------------------\n",
    "max_Evals=50000\n",
    "##--------------------------Tunning--------------------------\n",
    "max_evals_tunning=500\n",
    "trial_timeout_tunning=300\n",
    "\n",
    "\n",
    "#### Other classifires can be possibly used\n",
    "# andomForestClassifier(n_estimators=100,random_state=42)\n",
    "# AdaBoostClassifier()\n",
    "# ExtraTreesClassifier(n_estimators=50)\n",
    "# AdaBoostClassifier() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysxnCrczi6Yg"
   },
   "source": [
    "# **Methods and Classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJRbD9QCTCr0"
   },
   "source": [
    "## **Result, PrintResult,MakeDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:32.482715Z",
     "start_time": "2021-03-08T08:57:32.274308Z"
    },
    "id": "pW3GH0O-swu8"
   },
   "outputs": [],
   "source": [
    "class Result:\n",
    "    Score_train,Score_test,AUC_train,AUC_test,n_selceted_feature,n_Rules,state=0,0,0,0,0,0,''\n",
    "    def __init__(self, Score_train=0, Score_test=0,AUC_train=0,AUC_test=0,n_selceted_feature=0,n_Rules=0,state=''):\n",
    "        self.Score_train,self.Score_test,self.AUC_train,self.AUC_test,self.n_selceted_feature,self.n_Rules,self.state=Score_train,Score_test,AUC_train,AUC_test,n_selceted_feature,n_Rules,state\n",
    "\n",
    "def PrintResults(results, state,DF_results):\n",
    "  # print('\\n'+ state + '....')\n",
    "    scores = [r.Score_train for r in results if r.state==state]\n",
    "    if len(scores)==0:\n",
    "        S1=np.nan\n",
    "    else:\n",
    "        S1=sum(scores)/len(scores)\n",
    "    scores = [r.Score_test for r in results if r.state==state]\n",
    "    if len(scores)==0: \n",
    "        S2=np.nan \n",
    "        S8=np.nan\n",
    "    else:\n",
    "        S2=sum(scores)/len(scores)\n",
    "        S8=np.std(scores)\n",
    "    scores = [r.AUC_train for r in results if r.state==state]\n",
    "    if len(scores)==0:\n",
    "        S3=np.nan\n",
    "    else:\n",
    "        S3=sum(scores)/len(scores)\n",
    "    scores = [r.AUC_test for r in results if r.state==state]\n",
    "    if len(scores)==0:\n",
    "        S4=np.nan\n",
    "        S7=np.nan\n",
    "    else:\n",
    "        S4=sum(scores)/len(scores)\n",
    "        S7=np.std(scores)\n",
    "    NumOfFeatures = [r.n_selceted_feature for r in results if r.state==state]\n",
    "    if len(NumOfFeatures)==0:\n",
    "        S5=np.nan\n",
    "    else:\n",
    "        S5=sum(NumOfFeatures)/len(NumOfFeatures)       \n",
    "    NumOfRules = [r.n_Rules for r in results if r.state==state]   \n",
    "    if len(NumOfRules)==0:\n",
    "        S6=np.nan\n",
    "    else:\n",
    "        S6=sum(NumOfRules)/len(NumOfRules)\n",
    "        \n",
    "    new_row = {'State':state, 'Train Acc':round(S1*100,2),'Train AUC':round(S3*100,2), 'Test Acc':round(S2*100,2),'STD':round(S8*100,2), 'Test AUC':round(S4*100,2), 'STD': round(S7*100,2) ,'# Features':S5, '# Rules':S6}\n",
    "    DF_results = DF_results.append(new_row, ignore_index=True) \n",
    "  # print('\\n'+ state + '....')\n",
    "  # print('Average Score_train: {}'.format(S1) + '  Average Score_test : {}'.format(S2) + '  NumOfFeatures : {}'.format(S3))\n",
    "    return DF_results\n",
    "\n",
    "def MakeDF(data_array,old_df,cols):\n",
    "    # cols = old_df.columns[selector.get_support()]\n",
    "    new_df = pd.DataFrame(data_array, index=old_df.index, columns=cols)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-26ByJFcHLR"
   },
   "source": [
    "## **Train Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:32.717296Z",
     "start_time": "2021-03-08T08:57:32.484718Z"
    },
    "id": "48OAkPTeBzZ3"
   },
   "outputs": [],
   "source": [
    "# %%capture cap --no-stdout\n",
    "def Trainning(X_train,X_test,y_train,y_test,clf,state):\n",
    "    clf=clf.fit(X_train,y_train)\n",
    "    Score_train=clf.score(X_train,y_train)\n",
    "    predicted_prob_train=clf.predict_proba(X_train)\n",
    "    AUC_train= roc_auc_score(y_train, predicted_prob_train[:, 1])\n",
    "    \n",
    "    Score_test=clf.score(X_test,y_test)\n",
    "    predicted_prob_test=clf.predict_proba(X_test)\n",
    "    AUC_test = roc_auc_score(y_test, predicted_prob_test[:, 1])\n",
    "#     print(\"AUC_test = \"+str(AUC_test))    \n",
    "#     print(\"Initial Test ACC = \",Score_test)   \n",
    "    return  Result(Score_train,Score_test,AUC_train,AUC_test,X_train.shape[1],0,state)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8ZwEThXMAkI"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:32.828478Z",
     "start_time": "2021-03-08T08:57:32.719296Z"
    },
    "id": "Or2tg5ERcEY-"
   },
   "outputs": [],
   "source": [
    "def ensg_id_to_genes_name(data):\n",
    "    data.index.names = [\"genes_names\"]\n",
    "    gene_annot = pd.read_csv(ensembl_and_gene_names, \"\\t\", index_col=0)\n",
    "    gene_annot = gene_annot[['genes_names', 'ensembl_name2']]\n",
    "    gene_annot.index = gene_annot[['genes_names'][0]]\n",
    "    gene_annot = gene_annot.drop('genes_names', axis=1)\n",
    "    merged_data = pd.merge(gene_annot, data, left_index=True, right_index=True)\n",
    "    merged_data.index = merged_data[['ensembl_name2'][0]]\n",
    "    merged_data = merged_data.drop('ensembl_name2', axis=1)\n",
    "    return merged_data\n",
    "\n",
    "def merge_split_data_labels(data, labels):\n",
    "    merged_data = pd.merge(labels, data, left_index=True, right_index=True)\n",
    "    labels = merged_data.loc[:, [\"Classification.group\"]]\n",
    "    data = merged_data.drop(columns=['Classification.group'])\n",
    "    return data, labels\n",
    "\n",
    "def ReadData(data_path, labels_path):\n",
    "    #X, y = load_iris(return_X_y = True)\n",
    "#     df=pd.read_csv(Path_data)\n",
    "#     df_y=pd.read_csv(Path_label)\n",
    "#     data=df.iloc[:, 1:-1]\n",
    "#     labels=df_y.iloc[:, 2].replace('Non.cancer', 0).replace('NSCLC', 1)\n",
    "    \n",
    "    \n",
    "    data = pd.read_csv(data_path, sep=\" \", index_col=0)\n",
    "    labels = pd.read_csv(labels_path, sep=\",\", index_col=0)\n",
    "    labels = labels['Classification.group']\n",
    "    labels = labels.replace('Non-cancer', 0)\n",
    "    labels = labels.replace('NSCLC', 1)\n",
    "    \n",
    "    data = ensg_id_to_genes_name(data).transpose()\n",
    "    labels_cl = shuffle(labels, random_state=shuffle_random_seed)\n",
    "    data, labels_cl = merge_split_data_labels(data, labels_cl)\n",
    "    \n",
    "    return data,labels_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **------------Start Pipline------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:32.950391Z",
     "start_time": "2021-03-08T08:57:32.830480Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "from subprocess import call  \n",
    "import shlex  # for Linux\n",
    "\n",
    "results_folder = \"./R/results/\"\n",
    "r_scripts_path = \"./R/scripts/\"\n",
    "data_folder = \"./R/data/\"\n",
    "RscriptExe_path =\"C:/Program Files/R/R4/bin/Rscript \"\n",
    "ensembl_and_gene_names = \"./R/data/Homo_sapiens.GRCh19.longest.GSE89843.ready.gtf\"\n",
    "\n",
    "def diff_genes(t, l):\n",
    "    if not path.exists(results_folder + \"feature_selection\"):\n",
    "        makedirs(results_folder + \"feature_selection\")\n",
    "    if not path.exists(results_folder + \"feature_selection/diff_genes\"):\n",
    "        makedirs(results_folder + \"feature_selection/diff_genes\")\n",
    "    df_result = []\n",
    "\n",
    "    training_t = t.transpose()\n",
    "\n",
    "    training_t.to_csv(results_folder + \"feature_selection/diff_genes/data.csv\", sep=\",\")\n",
    "    l.to_csv(results_folder + \"feature_selection/diff_genes/labels.csv\", sep=\",\")\n",
    "\n",
    "    cmd =RscriptExe_path + r_scripts_path  + \"diff_genes.R \" + results_folder + \"feature_selection/diff_genes/data.csv\" + \\\n",
    "            \" \" + results_folder + \"feature_selection/diff_genes/labels.csv\" + \\\n",
    "            \" edger\" + \" \" + results_folder + \"feature_selection/diff_genes/ \" + str(i)\n",
    "#     with open(log.path, \"a\") as o:\n",
    "#     call(cmd) \n",
    "    call(shlex.split(cmd))\n",
    "    top_diff_genes = pd.read_csv(results_folder + \"feature_selection/diff_genes/\" + \"most_diff_genes_edger_\" + str(i) +\".txt\", sep=\" \", index_col=0)\n",
    "    top_diff_genes = top_diff_genes.apply(pd.to_numeric)\n",
    "    return top_diff_genes\n",
    "\n",
    "def compute_negative_control(data, nb_fold, forced):\n",
    "    if not path.exists(results_folder + \"negative_control_genes_myron_ruvseq_cv_fold_\" + str(nb_fold) + \".txt\") or forced:\n",
    "        var_dic = {}\n",
    "        for (columnName, columnData) in data.iteritems():\n",
    "            value = columnData.values\n",
    "            var = variation(value, axis = 0)\n",
    "            var_dic[columnName] = var\n",
    "\n",
    "        var_dic = {k: v for k, v in sorted(var_dic.items(), key=lambda item: item[1])}\n",
    "\n",
    "        negative_control_genes = []\n",
    "        for x in list(var_dic)[0:100]:\n",
    "            negative_control_genes.append(x)\n",
    "\n",
    "        outF = open(results_folder + \"negative_control_genes_myron_ruvseq_cv_fold_\" + str(nb_fold) + \".txt\", \"w\")\n",
    "        for line in negative_control_genes:\n",
    "            outF.write(line)\n",
    "            outF.write(\"\\n\")\n",
    "        outF.close()\n",
    "            \n",
    "def compute_negative_control_diff(data, labels, nb_fold, current_norm, forced):\n",
    "    if not path.exists(results_folder + \"negative_control_genes_myron_\" + current_norm + \"_fold_\" + str(nb_fold) +\".txt\") or forced:\n",
    "        df_genes = diff_genes(data, labels)\n",
    "        closest_to_0_gene = df_genes['logFC'].abs().idxmin()\n",
    "        closest_to_0_idx = df_genes.index.get_loc(closest_to_0_gene)\n",
    "        range_up = closest_to_0_idx + top_f\n",
    "        range_down = closest_to_0_idx - top_f\n",
    "        index_lst = list(df_genes.index)\n",
    "        outF = open(results_folder + \"negative_control_genes_myron_\" + current_norm + \"_fold_\" + str(nb_fold) +\".txt\", \"w\")\n",
    "        for k in range(range_down, range_up, 1):\n",
    "            outF.write(index_lst[k])\n",
    "            outF.write(\"\\n\")\n",
    "        outF.close()\n",
    "        \n",
    "def compute_diff(data, labels, nb_fold, current_norm, forced):\n",
    "    if not path.exists(results_folder + \"diff_genes_myron_\" + current_norm + \"_fold_\" + str(nb_fold) + \".csv\") or forced:\n",
    "        df_genes = diff_genes(data, labels)\n",
    "        df_genes.to_csv(results_folder + \"diff_genes_myron_\" + current_norm + \"_fold_\" + str(nb_fold) + \".csv\")\n",
    "        return df_genes\n",
    "    else:\n",
    "        df_genes = pd.read_csv(results_folder + \"diff_genes_myron_\" + current_norm + \"_fold_\" + str(nb_fold) +\".csv\", index_col=0)\n",
    "        return df_genes\n",
    "    \n",
    "def normalisation(data, nb_fold, typeset,forced, data_id=\"GSE89843\"):\n",
    "#     print(data.shape)\n",
    "    if not path.exists(results_folder + \"normalisation/\"):\n",
    "        makedirs(results_folder + \"normalisation/\")\n",
    "\n",
    "    if not path.exists(results_folder + \"normalisation/\" + data_id + \"_TEP_Count_Matrix_\" + normalisation_method_chosen + \"_\" + str(nb_fold) + \"_\" + typeset + \".txt\") or forced:\n",
    "        data = data.transpose()\n",
    "        data.to_csv(data_folder + data_id + \"_training_to_norm.csv\", sep=\" \")\n",
    "        cmd =RscriptExe_path + r_scripts_path + \"normalisation.R \" + data_folder + data_id + \"_training_to_norm.csv\" + \" \" + \\\n",
    "                normalisation_method_chosen + \" \" + results_folder + \"normalisation/\" + \" \" + data_id + \" \" + ensembl_and_gene_names + \" \" + str(nb_fold) + \" \" + typeset\n",
    "#         call(cmd)  \n",
    "        call(shlex.split(cmd))\n",
    "\n",
    "        data_norm = pd.read_csv(results_folder + \"normalisation/\" + data_id + \"_TEP_Count_Matrix_\" + normalisation_method_chosen + \"_\" + str(nb_fold) + \"_\" + typeset +  \".txt\", sep=\" \", index_col=0)\n",
    "        data_norm.transpose().to_csv(results_folder + \"normalisation/\" + data_id + \"_TEP_Count_Matrix_\" + normalisation_method_chosen + \"_\" + str(nb_fold) + \"_\" + typeset +  \".csv\", sep=\",\")\n",
    "        data_norm = data_norm.apply(pd.to_numeric)\n",
    "        data_norm = data_norm.transpose()\n",
    "        print(\"Nomalization done..\")\n",
    "    else:\n",
    "        data_norm = pd.read_csv(results_folder + \"normalisation/\" + data_id + \"_TEP_Count_Matrix_\" + normalisation_method_chosen + \"_\" + str(nb_fold) + \"_\" + typeset + \".csv\", sep=\",\", index_col=0)\n",
    "        data_norm = data_norm.apply(pd.to_numeric)\n",
    "        print(\"Nomalization was not done, only read the prevoius results..\")\n",
    "\n",
    "    return data_norm\n",
    "    \n",
    "def Normalize (X_train,X_test,y_train,nb_fold,normalization_selected):    \n",
    "        training=X_train\n",
    "        training_labels=y_train\n",
    "        test=X_test\n",
    "        \n",
    "#         log_path = results_folder + \"log.txt\"\n",
    "        data_id = \"GSE89843\"\n",
    "#         log = Log(log_path, data_id)\n",
    "        \n",
    "               \n",
    "        if normalization_selected == \"ruvseq_cv\":\n",
    "            compute_negative_control(training, nb_fold, forced=forced)\n",
    "        elif normalization_selected == \"ruvseq_diff\":\n",
    "            compute_negative_control_diff(training, training_labels, nb_fold, normalization_selected, forced=forced)\n",
    "        \n",
    "        \n",
    "        training_norm = normalisation(training, nb_fold, \"training\",forced)\n",
    "        index_2_remove = training_norm.columns[(training_norm == 0).all()].tolist()\n",
    "        training_norm = training_norm.drop(index_2_remove, axis=1)\n",
    "        test = test.drop(index_2_remove, axis=1)\n",
    "        df_genes = compute_diff(training, training_labels, nb_fold, normalization_selected, forced=forced)\n",
    "        t_mean = training_norm.mean(axis=0)\n",
    "        t_var = training_norm.var(axis=0)\n",
    "        training_norm = (training_norm - t_mean)/t_var\n",
    "        \n",
    "        test_norm = normalisation(test, nb_fold, \"test\",forced)\n",
    "        test_norm = (test_norm - t_mean)/t_var\n",
    "                \n",
    "        return training_norm,test_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pct5JcL2L274"
   },
   "source": [
    "# 1) Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:33.154311Z",
     "start_time": "2021-03-08T08:57:32.957382Z"
    },
    "id": "5qLvOdfxZOXU"
   },
   "outputs": [],
   "source": [
    "def Scaling(X_train,X_test):\n",
    "  ## 1.Scaling features to a range [0,1]\n",
    "  ## 2.Standardize features by removing the mean (u=0) and scaling to the same variance,z = (x - u) / s\n",
    "  scaler = MinMaxScaler().fit(X_train)\n",
    "  #scaler = StandardScaler(with_mean=False).fit(X_train)\n",
    "  X_train_new=scaler.transform(X_train)\n",
    "  X_test_new=scaler.transform(X_test)\n",
    "  selected_cols=X_train.columns\n",
    "  return MakeDF(X_train_new,X_train,selected_cols),MakeDF(X_test_new,X_test,selected_cols)   #return train_df,test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LQw0IUeLxPU"
   },
   "source": [
    "# **2) FS: K Best**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:33.314314Z",
     "start_time": "2021-03-08T08:57:33.167321Z"
    },
    "id": "ym_kufAAam7W"
   },
   "outputs": [],
   "source": [
    "def KBestFS_Fixed(X_train , X_test, y_train, metric_of_KBest, k_KBest):\n",
    "    selector = SelectKBest(metric_of_KBest, k=k_KBest)  \n",
    "    X_train_new=selector.fit(X_train, y_train).transform(X_train)\n",
    "    X_test_new=selector.transform(X_test)\n",
    "    selected_cols=X_train.columns.values[selector.get_support()]\n",
    "    return MakeDF(X_train_new,X_train,selected_cols),MakeDF(X_test_new,X_test,selected_cols)   #return train_df,test_df\n",
    "\n",
    "def KBestFS_Iterative(X_train,X_test, y_train,metric_of_KBest):\n",
    "        k_KBest=X_train.shape[1]\n",
    "        X_train_old,X_test_old=KBestFS_Fixed(X_train,X_test,y_train,metric_of_KBest,k_KBest)\n",
    "        result=Trainning(X_train_old,X_test_old,y_train,y_test,estimator_KBest_Iterative,'SelectKBest_'+str(k_KBest))\n",
    "        old_measure=result.Score_train\n",
    "        \n",
    "        while True:\n",
    "#             print('k_KBest '+str(k_KBest))\n",
    "#             print('old_measure '+str(old_measure))\n",
    "            k_old = k_KBest\n",
    "            k_KBest=math.floor(k_KBest/2)\n",
    "            X_train_new,X_test_new=KBestFS_Fixed(X_train_old,X_test_old,y_train,metric_of_KBest,k_KBest)\n",
    "            result=Trainning(X_train_new,X_test_new,y_train,y_test,estimator_KBest_Iterative,'SelectKBest_'+str(k_KBest))\n",
    "            new_measure=result.Score_train\n",
    "            \n",
    "            if (abs(new_measure-old_measure)>=0.02): \n",
    "                break\n",
    "            else :\n",
    "                old_measure=new_measure\n",
    "                X_train_old,X_test_old=X_train_new,X_test_new\n",
    "            \n",
    "        k_KBest=k_old\n",
    "        \n",
    "        while True:\n",
    "#             print('k_KBest '+str(k_KBest))\n",
    "#             print('old_measure '+str(old_measure))\n",
    "            k_KBest=k_KBest-100\n",
    "            X_train_new,X_test_new=KBestFS_Fixed(X_train_old,X_test_old,y_train,metric_of_KBest,k_KBest)\n",
    "            result=Trainning(X_train_new,X_test_new,y_train,y_test,estimator_KBest_Iterative,'SelectKBest_'+str(k_KBest))\n",
    "            new_measure=result.Score_train\n",
    "            \n",
    "            if (abs(new_measure-old_measure)>=0.01): \n",
    "                break\n",
    "            else :\n",
    "                old_measure=new_measure\n",
    "                X_train_old,X_test_old=X_train_new,X_test_new                                     \n",
    "        return X_train_old,X_test_old \n",
    "\n",
    "def KBest_FS(X_train , X_test, y_train, metric_of_KBest, k_KBest):\n",
    "    if mode_KBest =='iterative':\n",
    "        return KBestFS_Iterative(X_train,X_test, y_train,metric_of_KBest)\n",
    "    else:\n",
    "        return KBestFS_Fixed(X_train , X_test, y_train, metric_of_KBest, k_KBest)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2lKRY2MI2xX"
   },
   "source": [
    "# **3) FS: SelectFromModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:33.530441Z",
     "start_time": "2021-03-08T08:57:33.319316Z"
    },
    "id": "tnw33opv9d1C"
   },
   "outputs": [],
   "source": [
    "def ByModelFS(X_train , X_test, y_train):\n",
    "  selector = SelectFromModel(estimator_SelectFromModel,max_features=100)  \n",
    "  X_train_new=selector.fit(X_train, y_train).transform(X_train)\n",
    "  X_test_new=selector.transform(X_test)\n",
    "  selected_cols=X_train.columns.values[selector.get_support()]\n",
    "  return MakeDF(X_train_new,X_train,selected_cols),MakeDF(X_test_new,X_test,selected_cols)   #return train_df,test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2Hwo-PTLrjL"
   },
   "source": [
    "# **4) FS: RFE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:33.705577Z",
     "start_time": "2021-03-08T08:57:33.534440Z"
    },
    "id": "1vd3AcPxDWcc"
   },
   "outputs": [],
   "source": [
    "def RFE_FS(X_train , X_test,  y_train,n_final_features_to_select, step_REF):\n",
    "  selector = RFE(estimator_RFE, n_final_features_to_select, step=step_REF)\n",
    "  X_train_new=selector.fit(X_train, y_train).transform(X_train)\n",
    "  X_test_new=selector.transform(X_test)\n",
    "  selected_cols=X_train.columns.values[selector.get_support()]\n",
    "  return MakeDF(X_train_new,X_train,selected_cols),MakeDF(X_test_new,X_test,selected_cols)   #return train_df,test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xflHzBh47ATS"
   },
   "source": [
    "# **5) HyperoptEstimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T09:01:51.002429Z",
     "start_time": "2021-03-08T09:01:50.982418Z"
    },
    "id": "V_KqBsXm3faM"
   },
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "# Under Development......\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing,extra_trees\n",
    "from hyperopt import tpe\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import Trials\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "N_FOLDS = 5\n",
    "MAX_EVALS = 1000\n",
    "def objective(params, n_folds = N_FOLDS):\n",
    "        \"\"\"Objective function for Logistic Regression Hyperparameter Tuning\"\"\"\n",
    "\n",
    "        # Perform n_fold cross validation with hyperparameters\n",
    "        # Use early stopping and evaluate based on ROC AUC\n",
    "\n",
    "        clf = LogisticRegression(**params,random_state=0,verbose =0)\n",
    "        scores = cross_val_score(clf, X_train,y_train, cv=5, scoring='roc_auc')  #roc_auc   f1_macro\n",
    "\n",
    "        # Extract the best score\n",
    "        best_score = max(scores)\n",
    "\n",
    "        # Loss must be minimized\n",
    "        loss = 1 - best_score\n",
    "\n",
    "        # Dictionary with information for evaluation\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "    \n",
    "def Tuning_classifier():\n",
    "    space = {\n",
    "        #'class_weight': hp.choice('class_weight', [None, class_weight]),\n",
    "        'warm_start' : hp.choice('warm_start', [True, False]),\n",
    "        'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
    "        'tol' : hp.uniform('tol', 0.00001, 0.0001),\n",
    "        'C' : hp.uniform('C', 0.05, 3),\n",
    "        'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "        'max_iter' : hp.choice('max_iter', range(5,max_evals_tunning))}\n",
    "\n",
    "    # Algorithm\n",
    "    tpe_algorithm = tpe.suggest\n",
    "\n",
    "    # Trials object to track progress\n",
    "    bayes_trials = Trials()\n",
    "\n",
    "    # Optimize\n",
    "    best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)\n",
    "\n",
    "    # Fit-Predict\n",
    "    clf1= clf.fit(X_train, y_train)\n",
    "    return clf1\n",
    "\n",
    "\n",
    "def Tuning_classifier_2():\n",
    "    clf = hp.choice('clf', [(random_forest('random_forest', n_estimators=hp.choice('n_estimators_rf', range(1,100)),\n",
    "                                criterion=hp.choice('criterion_rf', [\"gini\", \"entropy\"]),\n",
    "                                n_jobs=10)),\n",
    "                                (extra_trees('extra_tree', n_estimators=hp.choice('n_estimators_extra', range(1,100)),\n",
    "                                criterion=hp.choice('criterion_extra', [\"gini\", \"entropy\"]),\n",
    "                                n_jobs=10))])\n",
    "\n",
    "    estim = HyperoptEstimator(classifier=clf,\n",
    "                                  preprocessing=[],\n",
    "                                  algo=tpe.suggest,\n",
    "                                  max_evals=max_evals_tunning,\n",
    "                                  trial_timeout=trial_timeout_tunning)\n",
    "    \n",
    "    \n",
    "    estim.fit(X_train,y_train)\n",
    "    clf = estim.best_model().get('learner')\n",
    "    return clf,estim.best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) ChiRWClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:34.338468Z",
     "start_time": "2021-03-08T08:57:33.842325Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/Implementation/ChiFRBCSPy\")\n",
    "    \n",
    "from ChiRWClassifier import ChiRWClassifier\n",
    "  \n",
    "def Traing_ChiRWClassifier(X_tr,X_tst,y_tr,y_tst,state):\n",
    "\n",
    "    start_time = time.time()\n",
    "    chi = ChiRWClassifier(frm=\"wr\")\n",
    "\n",
    "    chi=chi.fit(X_tr,y_tr)\n",
    "    y_pred = chi.predict(X_tr)\n",
    "    Score_train=accuracy_score(y_tr,y_pred)\n",
    "    print(\"The accuracy of Chi-FRBCS model (train) is: \",Score_train )\n",
    "    y_pred = chi.predict(X_tst)\n",
    "    Score_test=accuracy_score(y_tst,y_pred)\n",
    "    print(\"The accuracy of Chi-FRBCS model (test) is: \", Score_test)\n",
    "\n",
    "    #Only for two-class problems\n",
    "    probas_ = chi.predict_proba(X_tr)\n",
    "    fpr, tpr, thresholds = roc_curve(y_tr, probas_[:, 1])\n",
    "    AUC_train = auc(fpr, tpr)\n",
    "    print(\"The AUC of Chi-FRBCS model  (train) es: \", AUC_train)\n",
    "    \n",
    "    \n",
    "    probas_ = chi.predict_proba(X_tst)\n",
    "    fpr, tpr, thresholds = roc_curve(y_tst, probas_[:, 1])\n",
    "    AUC_test = auc(fpr, tpr)\n",
    "    print(\"The AUC of Chi-FRBCS model  (test) es: \", AUC_test)\n",
    "    \n",
    "    NumberofRules=len(chi.kb.ruleBase)\n",
    "\n",
    "    t_exec = time.time() - start_time\n",
    "    hours = int(t_exec / 3600);\n",
    "    rest = t_exec % 3600;\n",
    "    minutes = int(rest / 60);\n",
    "    seconds = rest % 60;\n",
    "\n",
    "    print(\"Execution Time: \", hours , \":\" , minutes , \":\" , '{0:.4g}'.format(seconds))\n",
    "    return  Result(Score_train,Score_test,AUC_train,AUC_test,X_train.shape[1],NumberofRules,state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMUN8ELacM6q"
   },
   "source": [
    "# 7) Trying SKMoefs (P.Ducange 2014)...\n",
    "https://github.com/GionatanG/skmoefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:42.023056Z",
     "start_time": "2021-03-08T08:57:34.340472Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%capture cap\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"C:/Users/MASNA.CO/skmoefs/skmoefs\")\n",
    "\n",
    "from platypus.algorithms import *\n",
    "from skmoefs.toolbox import MPAES_RCS, load_dataset, normalize\n",
    "from skmoefs.rcs import RCSInitializer, RCSVariator\n",
    "from skmoefs.discretization.discretizer_base import fuzzyDiscretization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def CreatDatFile(X,y,datastName):\n",
    "    df=X.copy()\n",
    "    df['Class']=y.copy()\n",
    "    f=open('dataset/' + datastName +'.dat', 'w')\n",
    "    f.write('@relation '+datastName +'\\n')\n",
    "    inputs=[]\n",
    "    line2=\"\"\n",
    "    for col in X.columns:\n",
    "        line='@attribute '+ col +' real ' +'['+ str(X[col].values.min()) +','+ str(X[col].values.max())+']\\n'\n",
    "        line2= line2+ str(col)+', '      \n",
    "        f.write(line) \n",
    "        \n",
    "    line='@attribute Class'+' integer ' +'{'+ str(y.min()) +','+ str(y.max())+'}\\n'\n",
    "\n",
    "    f.write(line)\n",
    "    f.write('@inputs '+line2+'\\n') \n",
    "    f.write('@outputs Class\\n@data\\n') \n",
    "    df.to_csv(f,index=False, header=False)\n",
    "    return \n",
    "\n",
    "def TrainMOEA(X_train,y_train,X_test,y_test,foldNumber,state,max_Evals):\n",
    "    dataset_Tra_Name='GeneExpre_Tra_'+str(foldNumber)\n",
    "    CreatDatFile(X_train,y_train,dataset_Tra_Name)\n",
    "    X_Tra, y_Tra, attributes, inputs, outputs = load_dataset(dataset_Tra_Name)  #iris\n",
    "    Xtr, ytr = normalize(X_Tra, y_Tra, attributes)\n",
    "\n",
    "    dataset_Tst_Name='GeneExpre_Tst_'+str(foldNumber)\n",
    "    CreatDatFile(X_test,y_test,dataset_Tst_Name)\n",
    "    X_Tst, y_Tst, attributes, inputs, outputs = load_dataset(dataset_Tst_Name)  #iris\n",
    "    Xte, yte = normalize(X_Tst, y_Tst, attributes)\n",
    "\n",
    "    my_moefs = MPAES_RCS(variator=RCSVariator(), initializer=RCSInitializer())\n",
    "    my_moefs.fit(Xtr, ytr, max_evals=max_Evals)\n",
    "\n",
    "    # my_moefs.show_pareto()\n",
    "    # my_moefs.show_pareto(Xte, yte)\n",
    "    # my_moefs.show_model('median', inputs=inputs, outputs=outputs)\n",
    "    scores_train=my_moefs.score(Xtr, ytr)\n",
    "    scores_test=my_moefs.score(Xte, yte)\n",
    "\n",
    "    Score_train = max(scores_train, key=lambda x:x[0])[0]  #0 accuracy  1 auc   2 trl\n",
    "    Score_test= max(scores_test, key=lambda x:x[0])[0]  #0 accuracy  1 auc   2 trl\n",
    "    AUC_train = max(Score_train, key=lambda x:x[1])[1]\n",
    "    AUC_test = max(scores_test, key=lambda x:x[1])[1]\n",
    "#     print(type(Score_train))\n",
    "#     print(Score_test)\n",
    "#     print(AUC)\n",
    "    return  Result(Score_train,Score_test,AUC_train,AUC_test ,X_train.shape[1],0,state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Trying Multi-Objective _FS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T08:57:43.038339Z",
     "start_time": "2021-03-08T08:57:42.024940Z"
    }
   },
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/Implementation/evofs\")\n",
    "    \n",
    "from evofs import EvoFS\n",
    "def EVOFS (X_train , X_test,y_train):\n",
    "\n",
    "        model = EvoFS(estimator=estimator_EVOFS,min_features=min_features,max_generations=max_generations,score_func=score_func,verbose=verbose)#\n",
    "#         model.fit(X_train.to_numpy(), y_train)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        x_train_reduced = model.transform(X_train)\n",
    "        x_test_reduced = model.transform(X_test)\n",
    "\n",
    "        return x_train_reduced ,x_test_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2V_uAeWfEgi"
   },
   "source": [
    "# **------------Running------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T10:02:43.685498Z",
     "start_time": "2021-03-13T07:35:46.224470Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwjyF7HDG6SS",
    "outputId": "6647cd57-5c03-42bb-a186-4ed44ed8cbba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold 1 is processing...\n",
      "Initialization is processing...\n",
      "Normalisation is processing...\n",
      "Nomalization done..\n",
      "Nomalization done..\n",
      "Scaling is processing...\n",
      "SelectKBest is processing...\n",
      "Multi-Objective_FS is processing...\n",
      "Fuzzy_Chi_Classifier is processing...\n",
      "Initializing classifier:\n",
      "\n",
      "Rule Generation\n",
      "Computing Matching Degrees Rule\n",
      "Computing Matching Degrees All\n",
      "Computing Rule Weights\n",
      "Rule Base: 362\n",
      "The accuracy of Chi-FRBCS model (train) is:  0.922879177377892\n",
      "The accuracy of Chi-FRBCS model (test) is:  0.735897435897436\n",
      "The AUC of Chi-FRBCS model  (train) es:  0.989255848417487\n",
      "The AUC of Chi-FRBCS model  (test) es:  0.7774355734554739\n",
      "Execution Time:  0 : 0 : 48.03\n",
      "Tuning_classifier is processing...\n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:18<00:00, 19.00s/trial, best loss: 0.2435897435897436]\n",
      "100%|██████████████████████████████████████████████████| 2/2 [00:20<00:00, 10.26s/trial, best loss: 0.1282051282051282]\n",
      "100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.23trial/s, best loss: 0.1282051282051282]\n",
      "100%|█████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.58trial/s, best loss: 0.11538461538461542]\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.23trial/s, best loss: 0.11538461538461542]\n",
      "100%|█████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.41trial/s, best loss: 0.08974358974358976]\n",
      "100%|█████████████████████████████████████████████████| 7/7 [00:02<00:00,  3.04trial/s, best loss: 0.08974358974358976]\n",
      "100%|█████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.67trial/s, best loss: 0.08974358974358976]\n",
      "100%|█████████████████████████████████████████████████| 9/9 [00:02<00:00,  3.54trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:02<00:00,  3.98trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 11/11 [00:02<00:00,  4.94trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 12/12 [00:02<00:00,  5.54trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 13/13 [00:02<00:00,  5.16trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 14/14 [00:02<00:00,  5.99trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 15/15 [00:02<00:00,  5.28trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 16/16 [00:02<00:00,  7.32trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 17/17 [00:02<00:00,  7.40trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 18/18 [00:02<00:00,  7.69trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 19/19 [00:02<00:00,  8.01trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 20/20 [00:02<00:00,  9.05trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 21/21 [00:02<00:00,  7.23trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 22/22 [00:02<00:00,  9.69trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 23/23 [00:02<00:00, 10.98trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 24/24 [00:02<00:00,  9.46trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 25/25 [00:02<00:00, 10.11trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 26/26 [00:03<00:00,  8.19trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 27/27 [00:02<00:00, 11.29trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 28/28 [00:02<00:00, 11.84trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 29/29 [00:02<00:00,  9.83trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 30/30 [00:02<00:00, 11.02trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 31/31 [00:02<00:00, 12.96trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 32/32 [00:02<00:00, 13.94trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 33/33 [00:02<00:00, 13.32trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 34/34 [00:02<00:00, 13.70trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 35/35 [00:02<00:00, 15.66trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 36/36 [00:02<00:00, 15.70trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 37/37 [00:02<00:00, 13.43trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 38/38 [00:02<00:00, 16.73trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 39/39 [00:03<00:00, 11.41trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 40/40 [00:08<00:00,  4.74trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 41/41 [00:08<00:00,  4.95trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 42/42 [00:09<00:00,  4.54trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 43/43 [00:06<00:00,  6.17trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 44/44 [00:02<00:00, 17.00trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 45/45 [00:02<00:00, 17.56trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 46/46 [00:02<00:00, 19.69trial/s, best loss: 0.08974358974358976]\n",
      "100%|███████████████████████████████████████████████| 47/47 [00:02<00:00, 19.11trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 48/48 [00:02<00:00, 21.54trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 49/49 [00:02<00:00, 21.44trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:02<00:00, 21.30trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 51/51 [00:02<00:00, 22.74trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 52/52 [00:02<00:00, 20.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 53/53 [00:02<00:00, 21.67trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 54/54 [00:02<00:00, 24.23trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 55/55 [00:02<00:00, 23.77trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 56/56 [00:02<00:00, 22.06trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 57/57 [00:02<00:00, 24.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 58/58 [00:02<00:00, 24.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 59/59 [00:02<00:00, 25.84trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 60/60 [00:02<00:00, 26.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 61/61 [00:02<00:00, 24.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 62/62 [00:02<00:00, 27.53trial/s, best loss: 0.07692307692307687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 63/63 [00:02<00:00, 23.40trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 64/64 [00:02<00:00, 27.98trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 65/65 [00:02<00:00, 26.92trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 66/66 [00:02<00:00, 28.35trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 67/67 [00:02<00:00, 28.52trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 68/68 [00:02<00:00, 29.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 69/69 [00:02<00:00, 29.11trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 70/70 [00:02<00:00, 30.34trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 71/71 [00:02<00:00, 29.81trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 72/72 [00:02<00:00, 30.91trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 73/73 [00:02<00:00, 31.53trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 74/74 [00:02<00:00, 33.32trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 75/75 [00:02<00:00, 32.60trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 76/76 [00:02<00:00, 32.46trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 77/77 [00:02<00:00, 29.59trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 78/78 [00:02<00:00, 29.79trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 79/79 [00:02<00:00, 35.24trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 80/80 [00:02<00:00, 33.44trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 81/81 [00:02<00:00, 33.61trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 82/82 [00:02<00:00, 31.74trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 83/83 [00:02<00:00, 38.22trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 84/84 [00:02<00:00, 34.03trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 85/85 [00:02<00:00, 33.65trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 86/86 [00:02<00:00, 36.84trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 87/87 [00:02<00:00, 37.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 88/88 [00:02<00:00, 38.27trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 89/89 [00:02<00:00, 39.50trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 90/90 [00:02<00:00, 39.21trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 91/91 [00:02<00:00, 38.18trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 92/92 [00:02<00:00, 38.41trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 93/93 [00:02<00:00, 41.45trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 94/94 [00:02<00:00, 40.00trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 95/95 [00:02<00:00, 44.44trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 96/96 [00:02<00:00, 43.01trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 97/97 [00:02<00:00, 40.74trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 98/98 [00:02<00:00, 43.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|███████████████████████████████████████████████| 99/99 [00:02<00:00, 36.18trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 100/100 [00:02<00:00, 41.28trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 101/101 [00:02<00:00, 40.08trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 102/102 [00:02<00:00, 43.99trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 103/103 [00:02<00:00, 41.98trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 104/104 [00:02<00:00, 43.73trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 105/105 [00:02<00:00, 45.72trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 106/106 [00:02<00:00, 45.32trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 107/107 [00:02<00:00, 42.63trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 108/108 [00:02<00:00, 46.47trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 109/109 [00:02<00:00, 43.54trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 110/110 [00:02<00:00, 41.16trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 111/111 [00:02<00:00, 41.94trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 112/112 [00:02<00:00, 49.88trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 113/113 [00:02<00:00, 44.53trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 114/114 [00:02<00:00, 43.70trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 115/115 [00:02<00:00, 45.72trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 116/116 [00:02<00:00, 41.39trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 117/117 [00:02<00:00, 46.84trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 118/118 [00:02<00:00, 40.83trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 119/119 [00:02<00:00, 42.58trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 120/120 [00:02<00:00, 50.13trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 121/121 [00:02<00:00, 43.72trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 122/122 [00:02<00:00, 45.27trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 123/123 [00:02<00:00, 52.54trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 124/124 [00:02<00:00, 54.82trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 125/125 [00:02<00:00, 43.27trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 126/126 [00:02<00:00, 50.46trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 127/127 [00:02<00:00, 48.43trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 128/128 [00:02<00:00, 52.75trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 129/129 [00:02<00:00, 47.06trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 130/130 [00:02<00:00, 55.72trial/s, best loss: 0.07692307692307687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 131/131 [00:02<00:00, 55.31trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 132/132 [00:02<00:00, 52.09trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 133/133 [00:02<00:00, 52.66trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 134/134 [00:02<00:00, 58.36trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 135/135 [00:02<00:00, 55.53trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 136/136 [00:02<00:00, 51.13trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 137/137 [00:02<00:00, 58.36trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 138/138 [00:03<00:00, 45.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 139/139 [00:02<00:00, 54.49trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 140/140 [00:02<00:00, 49.55trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 141/141 [00:02<00:00, 55.82trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 142/142 [00:02<00:00, 49.64trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 143/143 [00:02<00:00, 50.18trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 144/144 [00:02<00:00, 51.45trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 145/145 [00:02<00:00, 59.92trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 146/146 [00:02<00:00, 54.64trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 147/147 [00:02<00:00, 63.89trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 148/148 [00:02<00:00, 56.57trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 149/149 [00:02<00:00, 53.30trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 150/150 [00:02<00:00, 62.76trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 151/151 [00:02<00:00, 56.47trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 152/152 [00:02<00:00, 55.41trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 153/153 [00:02<00:00, 57.76trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 154/154 [00:02<00:00, 52.78trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 155/155 [00:02<00:00, 62.91trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 156/156 [00:02<00:00, 61.76trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 157/157 [00:02<00:00, 58.51trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 158/158 [00:02<00:00, 62.64trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 159/159 [00:02<00:00, 63.91trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 160/160 [00:02<00:00, 61.39trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 161/161 [00:02<00:00, 62.04trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 162/162 [00:02<00:00, 59.13trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 163/163 [00:02<00:00, 71.02trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 164/164 [00:02<00:00, 65.07trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 165/165 [00:02<00:00, 64.00trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 166/166 [00:02<00:00, 65.30trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 167/167 [00:02<00:00, 67.21trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 168/168 [00:02<00:00, 65.71trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 169/169 [00:02<00:00, 69.77trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 170/170 [00:02<00:00, 65.18trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 171/171 [00:02<00:00, 68.89trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 172/172 [00:02<00:00, 66.93trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 173/173 [00:02<00:00, 75.22trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 174/174 [00:02<00:00, 71.50trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 175/175 [00:02<00:00, 66.89trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 176/176 [00:02<00:00, 69.03trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 177/177 [00:02<00:00, 73.79trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 178/178 [00:02<00:00, 75.46trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 179/179 [00:02<00:00, 75.25trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 180/180 [00:02<00:00, 70.76trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 181/181 [00:02<00:00, 72.95trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 182/182 [00:02<00:00, 68.23trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 183/183 [00:02<00:00, 75.32trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 184/184 [00:02<00:00, 74.67trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 185/185 [00:02<00:00, 79.83trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 186/186 [00:02<00:00, 73.00trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 187/187 [00:02<00:00, 76.15trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 188/188 [00:02<00:00, 71.95trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 189/189 [00:02<00:00, 63.37trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 190/190 [00:02<00:00, 68.78trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 191/191 [00:02<00:00, 79.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 192/192 [00:02<00:00, 78.44trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 193/193 [00:02<00:00, 69.82trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 194/194 [00:02<00:00, 78.24trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 195/195 [00:02<00:00, 73.05trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 196/196 [00:02<00:00, 77.40trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 197/197 [00:02<00:00, 79.50trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 198/198 [00:02<00:00, 81.90trial/s, best loss: 0.07692307692307687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 199/199 [00:02<00:00, 72.89trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 200/200 [00:02<00:00, 76.43trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 201/201 [00:02<00:00, 73.78trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 202/202 [00:02<00:00, 75.05trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 203/203 [00:03<00:00, 65.55trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 204/204 [00:02<00:00, 76.13trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 205/205 [00:02<00:00, 84.84trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 206/206 [00:02<00:00, 85.19trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 207/207 [00:03<00:00, 66.42trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 208/208 [00:03<00:00, 57.49trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 209/209 [00:02<00:00, 88.65trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 210/210 [00:02<00:00, 81.87trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 211/211 [00:02<00:00, 75.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 212/212 [00:03<00:00, 63.19trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 213/213 [00:02<00:00, 79.50trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 214/214 [00:02<00:00, 91.71trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 215/215 [00:02<00:00, 84.84trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 216/216 [00:02<00:00, 92.70trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 217/217 [00:02<00:00, 93.19trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 218/218 [00:02<00:00, 88.01trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 219/219 [00:03<00:00, 67.23trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 220/220 [00:02<00:00, 89.76trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 221/221 [00:02<00:00, 87.62trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 222/222 [00:03<00:00, 66.63trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 223/223 [00:07<00:00, 30.16trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 224/224 [00:06<00:00, 34.84trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 225/225 [00:06<00:00, 36.34trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 226/226 [00:05<00:00, 43.40trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 227/227 [00:04<00:00, 48.95trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 228/228 [00:02<00:00, 84.53trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 229/229 [00:02<00:00, 87.14trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 230/230 [00:06<00:00, 34.86trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 231/231 [00:07<00:00, 30.59trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 232/232 [00:05<00:00, 40.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 233/233 [00:06<00:00, 33.99trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 234/234 [00:06<00:00, 37.46trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 235/235 [00:06<00:00, 35.08trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 236/236 [00:05<00:00, 42.66trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 237/237 [00:06<00:00, 38.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 238/238 [00:07<00:00, 33.45trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 239/239 [00:06<00:00, 35.27trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 240/240 [00:06<00:00, 38.77trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 241/241 [00:04<00:00, 59.95trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 242/242 [00:02<00:00, 86.94trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 243/243 [00:02<00:00, 96.46trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 244/244 [00:02<00:00, 97.20trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 245/245 [00:02<00:00, 97.08trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 246/246 [00:02<00:00, 94.72trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 247/247 [00:02<00:00, 102.10trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 248/248 [00:02<00:00, 95.43trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 249/249 [00:02<00:00, 103.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 250/250 [00:02<00:00, 103.47trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 251/251 [00:02<00:00, 102.54trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 252/252 [00:02<00:00, 100.02trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 253/253 [00:02<00:00, 86.01trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 254/254 [00:02<00:00, 96.47trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 255/255 [00:02<00:00, 107.57trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 256/256 [00:02<00:00, 108.31trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 257/257 [00:04<00:00, 62.08trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 258/258 [00:02<00:00, 105.14trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 259/259 [00:05<00:00, 51.64trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 260/260 [00:04<00:00, 53.97trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 261/261 [00:02<00:00, 99.40trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 262/262 [00:02<00:00, 93.44trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 263/263 [00:02<00:00, 93.73trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 264/264 [00:02<00:00, 104.86trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 265/265 [00:02<00:00, 104.60trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 266/266 [00:02<00:00, 103.09trial/s, best loss: 0.07692307692307687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 267/267 [00:02<00:00, 110.85trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 268/268 [00:02<00:00, 116.21trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 269/269 [00:02<00:00, 114.23trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 270/270 [00:02<00:00, 113.54trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 271/271 [00:02<00:00, 107.82trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 272/272 [00:02<00:00, 111.69trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 273/273 [00:02<00:00, 96.81trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 274/274 [00:02<00:00, 111.63trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 275/275 [00:02<00:00, 108.03trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 276/276 [00:02<00:00, 115.65trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 277/277 [00:02<00:00, 114.55trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 278/278 [00:03<00:00, 74.49trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 279/279 [00:03<00:00, 84.06trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 280/280 [00:03<00:00, 80.76trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 281/281 [00:02<00:00, 114.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 282/282 [00:02<00:00, 110.49trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 283/283 [00:02<00:00, 108.49trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 284/284 [00:02<00:00, 114.04trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 285/285 [00:02<00:00, 116.24trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 286/286 [00:02<00:00, 118.55trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 287/287 [00:02<00:00, 117.41trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 288/288 [00:02<00:00, 118.34trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 289/289 [00:02<00:00, 115.16trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 290/290 [00:02<00:00, 108.18trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 291/291 [00:02<00:00, 118.88trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 292/292 [00:02<00:00, 119.73trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 293/293 [00:02<00:00, 117.22trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 294/294 [00:02<00:00, 122.81trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 295/295 [00:02<00:00, 124.83trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 296/296 [00:03<00:00, 85.60trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 297/297 [00:02<00:00, 126.66trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 298/298 [00:02<00:00, 111.42trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 299/299 [00:02<00:00, 101.69trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 300/300 [00:02<00:00, 116.65trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 301/301 [00:02<00:00, 107.75trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 302/302 [00:03<00:00, 100.62trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 303/303 [00:02<00:00, 113.77trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 304/304 [00:02<00:00, 115.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 305/305 [00:03<00:00, 100.49trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 306/306 [00:02<00:00, 113.44trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 307/307 [00:02<00:00, 120.23trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 308/308 [00:02<00:00, 117.73trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 309/309 [00:02<00:00, 120.41trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 310/310 [00:02<00:00, 131.27trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 311/311 [00:02<00:00, 129.68trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 312/312 [00:02<00:00, 127.07trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 313/313 [00:02<00:00, 121.95trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 314/314 [00:02<00:00, 128.29trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 315/315 [00:02<00:00, 128.89trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 316/316 [00:02<00:00, 124.61trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 317/317 [00:02<00:00, 129.81trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 318/318 [00:02<00:00, 126.92trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 319/319 [00:02<00:00, 122.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 320/320 [00:02<00:00, 119.81trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 321/321 [00:02<00:00, 129.69trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 322/322 [00:03<00:00, 107.16trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 323/323 [00:03<00:00, 105.38trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 324/324 [00:02<00:00, 128.30trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 325/325 [00:02<00:00, 128.34trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 326/326 [00:02<00:00, 126.05trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 327/327 [00:02<00:00, 124.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 328/328 [00:02<00:00, 131.51trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 329/329 [00:02<00:00, 140.43trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 330/330 [00:02<00:00, 131.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 331/331 [00:02<00:00, 139.75trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 332/332 [00:02<00:00, 135.14trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 333/333 [00:02<00:00, 128.38trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 334/334 [00:02<00:00, 136.09trial/s, best loss: 0.07692307692307687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 335/335 [00:02<00:00, 126.87trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 336/336 [00:02<00:00, 135.67trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 337/337 [00:02<00:00, 136.61trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 338/338 [00:02<00:00, 132.45trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 339/339 [00:02<00:00, 134.29trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 340/340 [00:02<00:00, 133.69trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 341/341 [00:02<00:00, 136.49trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 342/342 [00:02<00:00, 125.86trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 343/343 [00:02<00:00, 144.89trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 344/344 [00:02<00:00, 139.56trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 345/345 [00:02<00:00, 141.10trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 346/346 [00:02<00:00, 136.42trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 347/347 [00:02<00:00, 124.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 348/348 [00:02<00:00, 136.60trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 349/349 [00:02<00:00, 137.76trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 350/350 [00:02<00:00, 133.79trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 351/351 [00:02<00:00, 146.32trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 352/352 [00:02<00:00, 140.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 353/353 [00:02<00:00, 147.34trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 354/354 [00:02<00:00, 144.02trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 355/355 [00:03<00:00, 112.91trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 356/356 [00:02<00:00, 135.66trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 357/357 [00:02<00:00, 144.18trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 358/358 [00:02<00:00, 138.05trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 359/359 [00:02<00:00, 150.10trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 360/360 [00:02<00:00, 149.53trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 361/361 [00:02<00:00, 156.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 362/362 [00:02<00:00, 151.17trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 363/363 [00:02<00:00, 146.38trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 364/364 [00:02<00:00, 141.75trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 365/365 [00:02<00:00, 147.02trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 366/366 [00:02<00:00, 152.18trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 367/367 [00:02<00:00, 144.72trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 368/368 [00:02<00:00, 146.44trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 369/369 [00:02<00:00, 147.57trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 370/370 [00:02<00:00, 155.77trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 371/371 [00:02<00:00, 151.30trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 372/372 [00:02<00:00, 149.45trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 373/373 [00:02<00:00, 148.02trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 374/374 [00:02<00:00, 135.97trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 375/375 [00:02<00:00, 147.63trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 376/376 [00:02<00:00, 150.28trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 377/377 [00:02<00:00, 153.50trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 378/378 [00:02<00:00, 145.95trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 379/379 [00:02<00:00, 150.64trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 380/380 [00:02<00:00, 151.25trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 381/381 [00:02<00:00, 153.15trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 382/382 [00:02<00:00, 154.65trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 383/383 [00:02<00:00, 164.29trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 384/384 [00:02<00:00, 151.71trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 385/385 [00:02<00:00, 150.25trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 386/386 [00:02<00:00, 149.97trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 387/387 [00:02<00:00, 155.74trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 388/388 [00:02<00:00, 131.29trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 389/389 [00:02<00:00, 146.51trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 390/390 [00:02<00:00, 134.69trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 391/391 [00:02<00:00, 150.82trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 392/392 [00:02<00:00, 162.48trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 393/393 [00:02<00:00, 157.36trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 394/394 [00:02<00:00, 157.86trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 395/395 [00:02<00:00, 160.67trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 396/396 [00:02<00:00, 144.54trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 397/397 [00:02<00:00, 159.59trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 398/398 [00:02<00:00, 150.38trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 399/399 [00:02<00:00, 160.64trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 400/400 [00:02<00:00, 157.47trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 401/401 [00:02<00:00, 161.76trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 402/402 [00:02<00:00, 162.24trial/s, best loss: 0.07692307692307687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 403/403 [00:02<00:00, 154.85trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 404/404 [00:02<00:00, 157.73trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 405/405 [00:02<00:00, 162.63trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 406/406 [00:02<00:00, 155.71trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 407/407 [00:02<00:00, 171.87trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 408/408 [00:02<00:00, 152.12trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 409/409 [00:02<00:00, 166.77trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 410/410 [00:02<00:00, 161.08trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 411/411 [00:02<00:00, 170.22trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 412/412 [00:02<00:00, 162.82trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 413/413 [00:03<00:00, 133.98trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 414/414 [00:02<00:00, 156.68trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 415/415 [00:02<00:00, 171.73trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 416/416 [00:02<00:00, 167.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 417/417 [00:02<00:00, 158.40trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 418/418 [00:02<00:00, 164.47trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 419/419 [00:02<00:00, 169.98trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 420/420 [00:02<00:00, 176.52trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 421/421 [00:02<00:00, 175.24trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 422/422 [00:02<00:00, 168.58trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 423/423 [00:02<00:00, 166.50trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 424/424 [00:02<00:00, 164.82trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 425/425 [00:02<00:00, 180.16trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 426/426 [00:02<00:00, 169.70trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 427/427 [00:02<00:00, 170.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 428/428 [00:02<00:00, 166.37trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 429/429 [00:02<00:00, 155.63trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 430/430 [00:02<00:00, 176.67trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 431/431 [00:02<00:00, 149.91trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 432/432 [00:02<00:00, 168.42trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 433/433 [00:02<00:00, 175.03trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 434/434 [00:02<00:00, 161.11trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 435/435 [00:02<00:00, 165.83trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 436/436 [00:02<00:00, 155.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 437/437 [00:02<00:00, 159.87trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 438/438 [00:02<00:00, 171.21trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 439/439 [00:02<00:00, 172.98trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 440/440 [00:02<00:00, 172.87trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 441/441 [00:02<00:00, 178.20trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 442/442 [00:02<00:00, 179.03trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 443/443 [00:02<00:00, 172.83trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 444/444 [00:02<00:00, 182.13trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 445/445 [00:02<00:00, 166.32trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 446/446 [00:02<00:00, 188.16trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 447/447 [00:02<00:00, 182.14trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 448/448 [00:02<00:00, 184.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 449/449 [00:02<00:00, 176.00trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 450/450 [00:02<00:00, 189.30trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 451/451 [00:02<00:00, 175.13trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 452/452 [00:02<00:00, 174.49trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 453/453 [00:02<00:00, 189.79trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 454/454 [00:02<00:00, 181.23trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 455/455 [00:02<00:00, 171.51trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 456/456 [00:02<00:00, 188.69trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 457/457 [00:02<00:00, 182.83trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 458/458 [00:02<00:00, 159.08trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 459/459 [00:02<00:00, 178.58trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 460/460 [00:02<00:00, 184.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 461/461 [00:02<00:00, 173.70trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 462/462 [00:02<00:00, 181.51trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 463/463 [00:02<00:00, 179.89trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 464/464 [00:02<00:00, 187.51trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 465/465 [00:02<00:00, 178.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 466/466 [00:02<00:00, 195.91trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 467/467 [00:02<00:00, 183.61trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 468/468 [00:02<00:00, 181.27trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 469/469 [00:02<00:00, 186.71trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 470/470 [00:02<00:00, 185.35trial/s, best loss: 0.07692307692307687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 471/471 [00:03<00:00, 151.62trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 472/472 [00:02<00:00, 189.93trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 473/473 [00:02<00:00, 185.35trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 474/474 [00:02<00:00, 169.97trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 475/475 [00:02<00:00, 190.77trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 476/476 [00:02<00:00, 182.16trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 477/477 [00:02<00:00, 177.03trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 478/478 [00:02<00:00, 185.92trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 479/479 [00:02<00:00, 179.70trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 480/480 [00:02<00:00, 180.93trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 481/481 [00:02<00:00, 180.46trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 482/482 [00:02<00:00, 186.24trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 483/483 [00:02<00:00, 184.19trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 484/484 [00:02<00:00, 188.39trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 485/485 [00:02<00:00, 191.10trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 486/486 [00:02<00:00, 192.39trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 487/487 [00:02<00:00, 198.08trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 488/488 [00:03<00:00, 160.14trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 489/489 [00:02<00:00, 199.24trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 490/490 [00:02<00:00, 205.07trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 491/491 [00:02<00:00, 195.39trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 492/492 [00:02<00:00, 191.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 493/493 [00:02<00:00, 198.58trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 494/494 [00:02<00:00, 181.30trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 495/495 [00:02<00:00, 183.69trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 496/496 [00:02<00:00, 186.83trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 497/497 [00:02<00:00, 181.93trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 498/498 [00:02<00:00, 195.85trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 499/499 [00:02<00:00, 194.78trial/s, best loss: 0.07692307692307687]\n",
      "100%|████████████████████████████████████████████| 500/500 [00:02<00:00, 184.31trial/s, best loss: 0.07692307692307687]\n",
      "\n",
      "fold 2 is processing...\n",
      "Initialization is processing...\n",
      "Normalisation is processing...\n",
      "Nomalization done..\n",
      "Nomalization done..\n",
      "Scaling is processing...\n",
      "SelectKBest is processing...\n",
      "Multi-Objective_FS is processing...\n",
      "Fuzzy_Chi_Classifier is processing...\n",
      "Initializing classifier:\n",
      "\n",
      "Rule Generation\n",
      "Computing Matching Degrees Rule\n",
      "Computing Matching Degrees All\n",
      "Computing Rule Weights\n",
      "Rule Base: 383\n",
      "The accuracy of Chi-FRBCS model (train) is:  0.9743589743589743\n",
      "The accuracy of Chi-FRBCS model (test) is:  0.7172236503856041\n",
      "The AUC of Chi-FRBCS model  (train) es:  0.9973939824686093\n",
      "The AUC of Chi-FRBCS model  (test) es:  0.7342145654705198\n",
      "Execution Time:  0 : 0 : 46.76\n",
      "Tuning_classifier is processing...\n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:29<00:00, 29.21s/trial, best loss: 0.17948717948717952]\n",
      "100%|█████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.34s/trial, best loss: 0.17948717948717952]\n",
      "100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.18trial/s, best loss: 0.1282051282051282]\n",
      "100%|██████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.75trial/s, best loss: 0.1282051282051282]\n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.14trial/s, best loss: 0.1282051282051282]\n",
      "100%|██████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.66trial/s, best loss: 0.1282051282051282]\n",
      "100%|██████████████████████████████████████████████████| 7/7 [00:02<00:00,  2.78trial/s, best loss: 0.1282051282051282]\n",
      "100%|██████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.83trial/s, best loss: 0.1282051282051282]\n",
      "100%|██████████████████████████████████████████████████| 9/9 [00:02<00:00,  4.31trial/s, best loss: 0.1282051282051282]\n",
      "100%|████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.66trial/s, best loss: 0.1282051282051282]\n",
      "100%|████████████████████████████████████████████████| 11/11 [00:02<00:00,  4.55trial/s, best loss: 0.1282051282051282]\n",
      "100%|████████████████████████████████████████████████| 12/12 [00:02<00:00,  5.10trial/s, best loss: 0.1282051282051282]\n",
      "100%|████████████████████████████████████████████████| 13/13 [00:02<00:00,  5.70trial/s, best loss: 0.1282051282051282]\n",
      "100%|███████████████████████████████████████████████| 14/14 [00:02<00:00,  6.48trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 15/15 [00:02<00:00,  5.60trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 16/16 [00:02<00:00,  6.66trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 17/17 [00:02<00:00,  7.83trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 18/18 [00:02<00:00,  8.50trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 19/19 [00:02<00:00,  7.83trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 20/20 [00:02<00:00,  9.38trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 21/21 [00:02<00:00,  9.22trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 22/22 [00:02<00:00,  8.57trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 23/23 [00:02<00:00,  8.23trial/s, best loss: 0.11538461538461542]\n",
      "100%|███████████████████████████████████████████████| 24/24 [00:02<00:00,  9.74trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 25/25 [00:02<00:00, 10.29trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 26/26 [00:02<00:00,  9.59trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 27/27 [00:02<00:00, 10.64trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 28/28 [00:02<00:00, 13.00trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 29/29 [00:02<00:00, 11.86trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 30/30 [00:02<00:00, 13.86trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 31/31 [00:02<00:00, 13.44trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 32/32 [00:02<00:00, 10.94trial/s, best loss: 0.10256410256410253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 33/33 [00:02<00:00, 14.89trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 34/34 [00:02<00:00, 14.03trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 35/35 [00:02<00:00, 15.29trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 36/36 [00:03<00:00, 11.66trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 37/37 [00:02<00:00, 16.00trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 38/38 [00:02<00:00, 16.15trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 39/39 [00:02<00:00, 15.43trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 40/40 [00:02<00:00, 18.37trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 41/41 [00:02<00:00, 16.67trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 42/42 [00:02<00:00, 17.73trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 43/43 [00:02<00:00, 18.75trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 44/44 [00:02<00:00, 20.55trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 45/45 [00:02<00:00, 19.91trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 46/46 [00:02<00:00, 19.49trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 47/47 [00:02<00:00, 20.14trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 48/48 [00:02<00:00, 21.33trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 49/49 [00:02<00:00, 21.99trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:03<00:00, 15.10trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 51/51 [00:02<00:00, 23.43trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 52/52 [00:02<00:00, 22.98trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 53/53 [00:02<00:00, 25.19trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 54/54 [00:02<00:00, 23.12trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 55/55 [00:02<00:00, 23.70trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 56/56 [00:02<00:00, 24.79trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 57/57 [00:02<00:00, 26.16trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 58/58 [00:02<00:00, 26.03trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 59/59 [00:02<00:00, 24.01trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 60/60 [00:02<00:00, 24.69trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 61/61 [00:02<00:00, 26.83trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 62/62 [00:02<00:00, 26.75trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 63/63 [00:02<00:00, 27.55trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 64/64 [00:02<00:00, 29.65trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 65/65 [00:02<00:00, 28.23trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 66/66 [00:02<00:00, 28.43trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 67/67 [00:02<00:00, 28.36trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 68/68 [00:02<00:00, 27.16trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 69/69 [00:02<00:00, 23.83trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 70/70 [00:02<00:00, 30.34trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 71/71 [00:02<00:00, 30.42trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 72/72 [00:02<00:00, 30.33trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 73/73 [00:02<00:00, 25.08trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 74/74 [00:02<00:00, 32.33trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 75/75 [00:02<00:00, 28.85trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 76/76 [00:02<00:00, 28.03trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 77/77 [00:02<00:00, 34.02trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 78/78 [00:02<00:00, 29.50trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 79/79 [00:02<00:00, 32.73trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 80/80 [00:02<00:00, 31.03trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 81/81 [00:02<00:00, 36.66trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 82/82 [00:02<00:00, 32.94trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 83/83 [00:02<00:00, 30.55trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 84/84 [00:02<00:00, 35.02trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 85/85 [00:02<00:00, 34.63trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 86/86 [00:02<00:00, 34.45trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 87/87 [00:02<00:00, 37.05trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 88/88 [00:02<00:00, 37.19trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 89/89 [00:02<00:00, 39.90trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 90/90 [00:03<00:00, 29.55trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 91/91 [00:02<00:00, 39.27trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 92/92 [00:02<00:00, 38.86trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 93/93 [00:02<00:00, 40.66trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 94/94 [00:02<00:00, 36.69trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 95/95 [00:02<00:00, 42.14trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 96/96 [00:02<00:00, 40.43trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 97/97 [00:02<00:00, 37.14trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 98/98 [00:02<00:00, 41.79trial/s, best loss: 0.10256410256410253]\n",
      "100%|███████████████████████████████████████████████| 99/99 [00:02<00:00, 40.15trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 100/100 [00:02<00:00, 47.47trial/s, best loss: 0.10256410256410253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 101/101 [00:02<00:00, 40.91trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 102/102 [00:02<00:00, 47.77trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 103/103 [00:02<00:00, 47.09trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 104/104 [00:02<00:00, 47.41trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 105/105 [00:02<00:00, 46.58trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 106/106 [00:02<00:00, 49.27trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 107/107 [00:02<00:00, 47.21trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 108/108 [00:02<00:00, 46.18trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 109/109 [00:02<00:00, 45.60trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 110/110 [00:02<00:00, 41.91trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 111/111 [00:02<00:00, 38.98trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 112/112 [00:02<00:00, 40.25trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 113/113 [00:02<00:00, 49.93trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 114/114 [00:02<00:00, 47.50trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 115/115 [00:02<00:00, 49.49trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 116/116 [00:02<00:00, 51.98trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 117/117 [00:02<00:00, 49.03trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 118/118 [00:02<00:00, 52.19trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 119/119 [00:02<00:00, 56.06trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 120/120 [00:02<00:00, 47.73trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 121/121 [00:02<00:00, 50.86trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 122/122 [00:02<00:00, 57.39trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 123/123 [00:02<00:00, 55.82trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 124/124 [00:02<00:00, 58.23trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 125/125 [00:02<00:00, 49.55trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 126/126 [00:02<00:00, 53.02trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 127/127 [00:02<00:00, 53.89trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 128/128 [00:02<00:00, 56.47trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 129/129 [00:02<00:00, 55.90trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 130/130 [00:02<00:00, 55.87trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 131/131 [00:02<00:00, 56.88trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 132/132 [00:02<00:00, 62.07trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 133/133 [00:02<00:00, 48.81trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 134/134 [00:02<00:00, 60.80trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 135/135 [00:02<00:00, 63.17trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 136/136 [00:02<00:00, 60.31trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 137/137 [00:02<00:00, 52.35trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 138/138 [00:02<00:00, 56.39trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 139/139 [00:02<00:00, 55.52trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 140/140 [00:02<00:00, 59.84trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 141/141 [00:02<00:00, 56.90trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 142/142 [00:02<00:00, 52.09trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 143/143 [00:02<00:00, 64.67trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 144/144 [00:02<00:00, 61.16trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 145/145 [00:02<00:00, 60.79trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 146/146 [00:02<00:00, 66.61trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 147/147 [00:02<00:00, 64.77trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 148/148 [00:02<00:00, 65.28trial/s, best loss: 0.10256410256410253]\n",
      "100%|█████████████████████████████████████████████| 149/149 [00:02<00:00, 63.29trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 150/150 [00:02<00:00, 68.48trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 151/151 [00:02<00:00, 68.80trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 152/152 [00:02<00:00, 59.45trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 153/153 [00:02<00:00, 67.10trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 154/154 [00:02<00:00, 68.56trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 155/155 [00:02<00:00, 66.20trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 156/156 [00:02<00:00, 67.40trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 157/157 [00:02<00:00, 66.10trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 158/158 [00:02<00:00, 68.26trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 159/159 [00:02<00:00, 67.96trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 160/160 [00:02<00:00, 67.75trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 161/161 [00:02<00:00, 69.00trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 162/162 [00:02<00:00, 66.77trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 163/163 [00:02<00:00, 59.34trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 164/164 [00:02<00:00, 69.79trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 165/165 [00:02<00:00, 68.50trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 166/166 [00:02<00:00, 70.11trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 167/167 [00:02<00:00, 67.87trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 168/168 [00:02<00:00, 77.99trial/s, best loss: 0.07692307692307687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 169/169 [00:02<00:00, 75.93trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 170/170 [00:02<00:00, 72.78trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 171/171 [00:02<00:00, 77.58trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 172/172 [00:02<00:00, 78.24trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 173/173 [00:02<00:00, 72.72trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 174/174 [00:02<00:00, 77.18trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 175/175 [00:02<00:00, 74.53trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 176/176 [00:02<00:00, 77.33trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 177/177 [00:02<00:00, 76.30trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 178/178 [00:02<00:00, 80.44trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 179/179 [00:02<00:00, 76.56trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 180/180 [00:02<00:00, 76.78trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 181/181 [00:02<00:00, 78.31trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 182/182 [00:02<00:00, 82.42trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 183/183 [00:02<00:00, 78.97trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 184/184 [00:02<00:00, 76.84trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 185/185 [00:02<00:00, 78.15trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 186/186 [00:02<00:00, 78.84trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 187/187 [00:02<00:00, 85.65trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 188/188 [00:02<00:00, 82.54trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 189/189 [00:02<00:00, 84.21trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 190/190 [00:02<00:00, 84.82trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 191/191 [00:02<00:00, 81.90trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 192/192 [00:02<00:00, 80.47trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 193/193 [00:02<00:00, 83.43trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 194/194 [00:02<00:00, 82.08trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 195/195 [00:02<00:00, 81.68trial/s, best loss: 0.07692307692307687]\n",
      "100%|█████████████████████████████████████████████| 196/196 [00:02<00:00, 78.96trial/s, best loss: 0.07692307692307687]\n",
      "100%|██████████████████████████████████████████████| 197/197 [00:02<00:00, 69.40trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 198/198 [00:02<00:00, 88.18trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 199/199 [00:02<00:00, 79.56trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 200/200 [00:02<00:00, 77.29trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 201/201 [00:02<00:00, 81.29trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 202/202 [00:02<00:00, 83.52trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 203/203 [00:02<00:00, 82.00trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 204/204 [00:02<00:00, 80.80trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 205/205 [00:02<00:00, 81.84trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 206/206 [00:02<00:00, 81.30trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 207/207 [00:02<00:00, 82.17trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 208/208 [00:02<00:00, 86.30trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 209/209 [00:02<00:00, 87.38trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 210/210 [00:02<00:00, 80.05trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 211/211 [00:02<00:00, 90.90trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 212/212 [00:02<00:00, 82.97trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 213/213 [00:02<00:00, 85.34trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 214/214 [00:02<00:00, 94.95trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 215/215 [00:02<00:00, 82.28trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 216/216 [00:02<00:00, 84.12trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 217/217 [00:02<00:00, 89.34trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 218/218 [00:02<00:00, 93.87trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 219/219 [00:02<00:00, 96.96trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 220/220 [00:02<00:00, 95.81trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 221/221 [00:02<00:00, 96.24trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 222/222 [00:02<00:00, 85.61trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 223/223 [00:02<00:00, 86.51trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 224/224 [00:02<00:00, 87.54trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 225/225 [00:02<00:00, 91.69trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 226/226 [00:02<00:00, 82.47trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 227/227 [00:02<00:00, 94.22trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 228/228 [00:02<00:00, 101.54trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 229/229 [00:02<00:00, 93.33trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 230/230 [00:02<00:00, 96.35trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 231/231 [00:02<00:00, 99.68trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 232/232 [00:02<00:00, 100.89trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 233/233 [00:02<00:00, 103.70trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 234/234 [00:02<00:00, 102.28trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 235/235 [00:02<00:00, 96.66trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 236/236 [00:02<00:00, 96.46trial/s, best loss: 0.0641025641025641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 237/237 [00:02<00:00, 101.23trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 238/238 [00:02<00:00, 88.46trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 239/239 [00:02<00:00, 103.53trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 240/240 [00:02<00:00, 98.84trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 241/241 [00:02<00:00, 100.11trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 242/242 [00:02<00:00, 110.59trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 243/243 [00:02<00:00, 102.82trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 244/244 [00:02<00:00, 105.60trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 245/245 [00:02<00:00, 102.02trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 246/246 [00:02<00:00, 105.60trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 247/247 [00:02<00:00, 110.27trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 248/248 [00:02<00:00, 106.90trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 249/249 [00:02<00:00, 109.52trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 250/250 [00:02<00:00, 103.19trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 251/251 [00:02<00:00, 110.29trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 252/252 [00:02<00:00, 110.97trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 253/253 [00:02<00:00, 121.04trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 254/254 [00:02<00:00, 112.88trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 255/255 [00:02<00:00, 94.97trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 256/256 [00:02<00:00, 114.56trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 257/257 [00:02<00:00, 108.85trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 258/258 [00:02<00:00, 116.91trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 259/259 [00:02<00:00, 121.98trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 260/260 [00:04<00:00, 63.82trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 261/261 [00:02<00:00, 110.64trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 262/262 [00:02<00:00, 112.67trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 263/263 [00:02<00:00, 109.09trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 264/264 [00:08<00:00, 32.02trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 265/265 [00:08<00:00, 31.21trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 266/266 [00:16<00:00, 16.44trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 267/267 [00:05<00:00, 45.86trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 268/268 [00:04<00:00, 59.97trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 269/269 [00:07<00:00, 36.19trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 270/270 [00:07<00:00, 34.76trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 271/271 [00:24<00:00, 10.90trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 272/272 [00:29<00:00,  9.22trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 273/273 [00:02<00:00, 101.25trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 274/274 [00:03<00:00, 86.33trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 275/275 [00:02<00:00, 103.23trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 276/276 [00:03<00:00, 89.61trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 277/277 [00:02<00:00, 103.92trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 278/278 [00:02<00:00, 118.09trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 279/279 [00:02<00:00, 115.37trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 280/280 [00:02<00:00, 111.75trial/s, best loss: 0.0641025641025641]\n",
      "100%|██████████████████████████████████████████████| 281/281 [00:03<00:00, 91.88trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 282/282 [00:02<00:00, 115.96trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 283/283 [00:02<00:00, 123.74trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 284/284 [00:02<00:00, 130.12trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 285/285 [00:02<00:00, 119.68trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 286/286 [00:02<00:00, 117.41trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 287/287 [00:02<00:00, 131.42trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 288/288 [00:02<00:00, 121.48trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 289/289 [00:02<00:00, 126.00trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 290/290 [00:02<00:00, 119.14trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 291/291 [00:02<00:00, 129.54trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 292/292 [00:02<00:00, 125.18trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 293/293 [00:02<00:00, 129.96trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 294/294 [00:02<00:00, 126.99trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 295/295 [00:02<00:00, 139.38trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 296/296 [00:02<00:00, 128.33trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 297/297 [00:02<00:00, 123.25trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 298/298 [00:02<00:00, 131.34trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 299/299 [00:02<00:00, 132.08trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 300/300 [00:02<00:00, 133.14trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 301/301 [00:02<00:00, 127.58trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 302/302 [00:02<00:00, 132.72trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 303/303 [00:02<00:00, 135.66trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 304/304 [00:02<00:00, 126.28trial/s, best loss: 0.0641025641025641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 305/305 [00:02<00:00, 131.05trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 306/306 [00:02<00:00, 135.46trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 307/307 [00:02<00:00, 137.36trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 308/308 [00:02<00:00, 133.67trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 309/309 [00:02<00:00, 128.33trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 310/310 [00:02<00:00, 135.64trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 311/311 [00:02<00:00, 138.32trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 312/312 [00:02<00:00, 125.30trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 313/313 [00:02<00:00, 132.68trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 314/314 [00:02<00:00, 134.57trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 315/315 [00:02<00:00, 137.25trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 316/316 [00:02<00:00, 138.61trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 317/317 [00:02<00:00, 133.60trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 318/318 [00:02<00:00, 134.13trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 319/319 [00:02<00:00, 134.01trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 320/320 [00:02<00:00, 136.77trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 321/321 [00:02<00:00, 134.26trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 322/322 [00:02<00:00, 135.96trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 323/323 [00:02<00:00, 131.62trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 324/324 [00:02<00:00, 136.18trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 325/325 [00:02<00:00, 114.18trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 326/326 [00:02<00:00, 149.76trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 327/327 [00:02<00:00, 136.89trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 328/328 [00:02<00:00, 138.82trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 329/329 [00:02<00:00, 149.76trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 330/330 [00:02<00:00, 152.04trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 331/331 [00:02<00:00, 137.06trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 332/332 [00:02<00:00, 146.32trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 333/333 [00:02<00:00, 144.65trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 334/334 [00:02<00:00, 139.06trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 335/335 [00:02<00:00, 148.92trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 336/336 [00:02<00:00, 146.35trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 337/337 [00:02<00:00, 158.40trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 338/338 [00:02<00:00, 146.39trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 339/339 [00:02<00:00, 148.32trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 340/340 [00:02<00:00, 145.41trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 341/341 [00:02<00:00, 147.96trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 342/342 [00:02<00:00, 124.99trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 343/343 [00:02<00:00, 154.32trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 344/344 [00:02<00:00, 156.33trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 345/345 [00:02<00:00, 148.78trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 346/346 [00:02<00:00, 152.34trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 347/347 [00:02<00:00, 150.43trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 348/348 [00:02<00:00, 155.24trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 349/349 [00:02<00:00, 147.68trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 350/350 [00:02<00:00, 144.76trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 351/351 [00:02<00:00, 146.86trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 352/352 [00:02<00:00, 154.87trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 353/353 [00:02<00:00, 155.46trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 354/354 [00:02<00:00, 140.43trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 355/355 [00:02<00:00, 164.52trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 356/356 [00:02<00:00, 154.70trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 357/357 [00:02<00:00, 154.15trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 358/358 [00:02<00:00, 147.32trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 359/359 [00:02<00:00, 136.26trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 360/360 [00:02<00:00, 151.59trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 361/361 [00:02<00:00, 151.28trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 362/362 [00:02<00:00, 150.03trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 363/363 [00:02<00:00, 162.67trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 364/364 [00:02<00:00, 160.02trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 365/365 [00:02<00:00, 150.59trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 366/366 [00:02<00:00, 148.14trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 367/367 [00:02<00:00, 141.80trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 368/368 [00:02<00:00, 174.74trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 369/369 [00:02<00:00, 148.74trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 370/370 [00:02<00:00, 159.18trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 371/371 [00:02<00:00, 155.33trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 372/372 [00:02<00:00, 159.11trial/s, best loss: 0.0641025641025641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 373/373 [00:02<00:00, 170.61trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 374/374 [00:02<00:00, 157.43trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 375/375 [00:02<00:00, 162.80trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 376/376 [00:02<00:00, 169.84trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 377/377 [00:02<00:00, 159.25trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 378/378 [00:02<00:00, 166.07trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 379/379 [00:02<00:00, 174.73trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 380/380 [00:02<00:00, 157.66trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 381/381 [00:02<00:00, 180.63trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 382/382 [00:02<00:00, 167.74trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 383/383 [00:02<00:00, 163.43trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 384/384 [00:02<00:00, 172.45trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 385/385 [00:02<00:00, 173.27trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 386/386 [00:02<00:00, 168.34trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 387/387 [00:02<00:00, 162.33trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 388/388 [00:02<00:00, 172.77trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 389/389 [00:02<00:00, 162.94trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 390/390 [00:02<00:00, 176.92trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 391/391 [00:02<00:00, 165.04trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 392/392 [00:02<00:00, 170.75trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 393/393 [00:02<00:00, 146.65trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 394/394 [00:02<00:00, 159.53trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 395/395 [00:02<00:00, 168.72trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 396/396 [00:02<00:00, 182.93trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 397/397 [00:02<00:00, 185.23trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 398/398 [00:02<00:00, 165.25trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 399/399 [00:02<00:00, 175.03trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 400/400 [00:02<00:00, 179.65trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 401/401 [00:02<00:00, 170.24trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 402/402 [00:02<00:00, 175.59trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 403/403 [00:02<00:00, 186.77trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 404/404 [00:02<00:00, 165.45trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 405/405 [00:02<00:00, 170.05trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 406/406 [00:02<00:00, 179.93trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 407/407 [00:02<00:00, 179.08trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 408/408 [00:02<00:00, 171.52trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 409/409 [00:02<00:00, 173.07trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 410/410 [00:02<00:00, 143.20trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 411/411 [00:02<00:00, 177.18trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 412/412 [00:02<00:00, 171.54trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 413/413 [00:02<00:00, 182.56trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 414/414 [00:02<00:00, 180.33trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 415/415 [00:02<00:00, 184.76trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 416/416 [00:02<00:00, 186.08trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 417/417 [00:02<00:00, 182.80trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 418/418 [00:02<00:00, 177.95trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 419/419 [00:02<00:00, 196.25trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 420/420 [00:02<00:00, 188.12trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 421/421 [00:02<00:00, 180.32trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 422/422 [00:02<00:00, 184.38trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 423/423 [00:02<00:00, 195.31trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 424/424 [00:02<00:00, 177.79trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 425/425 [00:02<00:00, 179.01trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 426/426 [00:02<00:00, 198.02trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 427/427 [00:02<00:00, 175.67trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 428/428 [00:02<00:00, 177.96trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 429/429 [00:02<00:00, 190.46trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 430/430 [00:02<00:00, 177.30trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 431/431 [00:02<00:00, 175.55trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 432/432 [00:02<00:00, 192.25trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 433/433 [00:02<00:00, 193.55trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 434/434 [00:02<00:00, 180.77trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 435/435 [00:02<00:00, 162.03trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 436/436 [00:02<00:00, 184.27trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 437/437 [00:02<00:00, 184.89trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 438/438 [00:02<00:00, 199.93trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 439/439 [00:02<00:00, 204.28trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 440/440 [00:02<00:00, 183.55trial/s, best loss: 0.0641025641025641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 441/441 [00:02<00:00, 192.74trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 442/442 [00:02<00:00, 190.33trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 443/443 [00:02<00:00, 198.65trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 444/444 [00:02<00:00, 194.10trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 445/445 [00:02<00:00, 190.78trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 446/446 [00:02<00:00, 190.69trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 447/447 [00:02<00:00, 198.01trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 448/448 [00:02<00:00, 189.26trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 449/449 [00:02<00:00, 198.43trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 450/450 [00:02<00:00, 201.30trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 451/451 [00:02<00:00, 204.20trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 452/452 [00:02<00:00, 179.85trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 453/453 [00:02<00:00, 183.34trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 454/454 [00:02<00:00, 183.05trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 455/455 [00:02<00:00, 196.23trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 456/456 [00:02<00:00, 187.15trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 457/457 [00:02<00:00, 188.38trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 458/458 [00:02<00:00, 196.28trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 459/459 [00:02<00:00, 209.76trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 460/460 [00:02<00:00, 196.88trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 461/461 [00:02<00:00, 184.72trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 462/462 [00:02<00:00, 189.61trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 463/463 [00:02<00:00, 215.29trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 464/464 [00:02<00:00, 196.10trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 465/465 [00:02<00:00, 200.09trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 466/466 [00:02<00:00, 195.83trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 467/467 [00:02<00:00, 187.27trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 468/468 [00:02<00:00, 198.07trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 469/469 [00:03<00:00, 140.69trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 470/470 [00:02<00:00, 162.69trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 471/471 [00:02<00:00, 214.18trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 472/472 [00:02<00:00, 212.17trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 473/473 [00:02<00:00, 200.18trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 474/474 [00:02<00:00, 192.10trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 475/475 [00:02<00:00, 211.65trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 476/476 [00:02<00:00, 192.35trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 477/477 [00:02<00:00, 217.55trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 478/478 [00:02<00:00, 187.19trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 479/479 [00:02<00:00, 216.51trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 480/480 [00:02<00:00, 220.20trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 481/481 [00:02<00:00, 199.79trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 482/482 [00:02<00:00, 211.97trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 483/483 [00:02<00:00, 185.37trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 484/484 [00:02<00:00, 207.83trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 485/485 [00:02<00:00, 218.71trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 486/486 [00:02<00:00, 177.73trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 487/487 [00:02<00:00, 200.66trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 488/488 [00:02<00:00, 203.62trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 489/489 [00:02<00:00, 213.43trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 490/490 [00:02<00:00, 214.43trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 491/491 [00:02<00:00, 206.88trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 492/492 [00:02<00:00, 210.35trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 493/493 [00:02<00:00, 212.72trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 494/494 [00:02<00:00, 220.54trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 495/495 [00:02<00:00, 196.78trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 496/496 [00:02<00:00, 198.32trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 497/497 [00:02<00:00, 219.27trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 498/498 [00:02<00:00, 221.42trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 499/499 [00:02<00:00, 219.75trial/s, best loss: 0.0641025641025641]\n",
      "100%|█████████████████████████████████████████████| 500/500 [00:02<00:00, 211.42trial/s, best loss: 0.0641025641025641]\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stdout\n",
    "results=[]\n",
    "Best_model=[]\n",
    "Best_classifier=[]\n",
    "i=0\n",
    "X,y = ReadData(data_path, labels_path)\n",
    "\n",
    "# cv= KFold(n_folds, random_state=42, shuffle=True) \n",
    "# for train_index, test_index in cv.split(X):\n",
    "\n",
    "cv = StratifiedKFold(n_folds, random_state=42, shuffle=True)#\n",
    "for train_index, test_index in cv.split(X,y):\n",
    "    i=i+1\n",
    "    print('\\nfold '+str(i)+' is processing...')\n",
    "    \n",
    "    X_train,X_test,y_train,y_test=X.iloc[train_index,:],X.iloc[test_index,:],y.iloc[train_index],y.iloc[test_index]\n",
    "    \n",
    "    print('Initialization is processing...')\n",
    "    result=Trainning(X_train,X_test,y_train,y_test,clf,'Initialization')\n",
    "    results.append(result)\n",
    "    \n",
    "    print('Normalisation is processing...')\n",
    "    X_train, X_test = Normalize(X_train,X_test,y_train,i,normalization_selected)\n",
    "    result=Trainning(X_train,X_test,y_train,y_test,clf,'Normalisation_'+normalization_selected)\n",
    "    results.append(result)    \n",
    "\n",
    "    print('Scaling is processing...')\n",
    "    X_train,X_test=Scaling(X_train,X_test)\n",
    "    result=Trainning(X_train,X_test,y_train,y_test,clf,'Scaling')\n",
    "    results.append(result)\n",
    "\n",
    "    print('SelectKBest is processing...')\n",
    "    X_train,X_test=KBest_FS(X_train , X_test, y_train, metric_of_KBest, k_KBest)\n",
    "    result=Trainning(X_train,X_test,y_train,y_test,clf,mode_KBest+'_KBest')\n",
    "    results.append(result)\n",
    "\n",
    "#     print('SelectFromModel is processing...')\n",
    "#     X_train,X_test=ByModelFS(X_train,X_test, y_train)\n",
    "#     result=Trainning(X_train,X_test,y_train,y_test,clf,'SelectFromModel')\n",
    "#     results.append(result)\n",
    "\n",
    "#     print('RFE_FS is processing...')\n",
    "#     X_train,X_test=RFE_FS(X_train , X_test, y_train, n_final_features_to_select, step_REF)\n",
    "#     result=Trainning(X_train,X_test,y_train,y_test,clf,'RFE_FS')\n",
    "#     results.append(result)\n",
    "\n",
    "    print('Multi-Objective_FS is processing...')\n",
    "    X_train,X_test= EVOFS (X_train , X_test,y_train['Classification.group'].values)\n",
    "    result=Trainning(X_train,X_test,y_train,y_test,clf,'Multi-Objective_FS')\n",
    "    results.append(result) \n",
    "\n",
    "    print('Fuzzy_Chi_Classifier is processing...')\n",
    "    result=Traing_ChiRWClassifier(X_train,X_test,y_train,y_test,'Fuzzy_Chi_Classifier')\n",
    "    results.append(result)\n",
    "\n",
    "#     print('MPAES_RCS is processing...')\n",
    "#     result= TrainMOEA(X_train,y_train,X_test,y_test,i-1,\"MPAES_RCS\",max_Evals)\n",
    "#     results.append(result)\n",
    "\n",
    "    print('Tuning_classifier is processing...')\n",
    "    clf1, model=Tuning_classifier_2()  # random_forest() or  extra_trees()\n",
    "#     clf1=Tuning_classifier()  # logistic tuning\n",
    "    result=Trainning(X_train,X_test,y_train,y_test,clf1,'Tuning_classifier')\n",
    "    results.append(result)\n",
    "    Best_model.append(model)\n",
    "    Best_classifier.append(clf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAkO_lPqfexC"
   },
   "source": [
    "# **Show Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T10:05:04.540567Z",
     "start_time": "2021-03-13T10:05:02.446556Z"
    },
    "id": "bGkT2aM1nq28"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Train AUC</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>STD</th>\n",
       "      <th>Test AUC</th>\n",
       "      <th>STD</th>\n",
       "      <th># Features</th>\n",
       "      <th># Rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Initialization</td>\n",
       "      <td>86.91</td>\n",
       "      <td>92.75</td>\n",
       "      <td>83.05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>87.29</td>\n",
       "      <td>2.21</td>\n",
       "      <td>4587.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Normalisation_TPM</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>84.08</td>\n",
       "      <td>1.53</td>\n",
       "      <td>92.14</td>\n",
       "      <td>1.53</td>\n",
       "      <td>4587.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Scaling</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>89.60</td>\n",
       "      <td>1.10</td>\n",
       "      <td>95.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>4587.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>iterative_KBest</td>\n",
       "      <td>98.07</td>\n",
       "      <td>99.75</td>\n",
       "      <td>88.45</td>\n",
       "      <td>0.16</td>\n",
       "      <td>94.31</td>\n",
       "      <td>0.16</td>\n",
       "      <td>409.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Multi-Objective_FS</td>\n",
       "      <td>89.34</td>\n",
       "      <td>94.89</td>\n",
       "      <td>82.93</td>\n",
       "      <td>0.81</td>\n",
       "      <td>89.89</td>\n",
       "      <td>0.81</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Fuzzy_Chi_Classifier</td>\n",
       "      <td>94.86</td>\n",
       "      <td>99.33</td>\n",
       "      <td>72.66</td>\n",
       "      <td>2.16</td>\n",
       "      <td>75.58</td>\n",
       "      <td>2.16</td>\n",
       "      <td>40.0</td>\n",
       "      <td>372.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Tuning_classifier</td>\n",
       "      <td>99.10</td>\n",
       "      <td>99.92</td>\n",
       "      <td>82.03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>90.06</td>\n",
       "      <td>0.23</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  State  Train Acc  Train AUC  Test Acc   STD  Test AUC   STD  \\\n",
       "0        Initialization      86.91      92.75     83.05  2.21     87.29  2.21   \n",
       "1     Normalisation_TPM     100.00     100.00     84.08  1.53     92.14  1.53   \n",
       "2               Scaling     100.00     100.00     89.60  1.10     95.80  1.10   \n",
       "3       iterative_KBest      98.07      99.75     88.45  0.16     94.31  0.16   \n",
       "4    Multi-Objective_FS      89.34      94.89     82.93  0.81     89.89  0.81   \n",
       "5  Fuzzy_Chi_Classifier      94.86      99.33     72.66  2.16     75.58  2.16   \n",
       "6     Tuning_classifier      99.10      99.92     82.03  0.23     90.06  0.23   \n",
       "\n",
       "   # Features  # Rules  \n",
       "0      4587.0      0.0  \n",
       "1      4587.0      0.0  \n",
       "2      4587.0      0.0  \n",
       "3       409.5      0.0  \n",
       "4        40.0      0.0  \n",
       "5        40.0    372.5  \n",
       "6        40.0      0.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results_df = pd.DataFrame(columns=['State', 'Train Acc', 'Train AUC','Test Acc','STD','Test AUC','STD','# Features','# Rules'])\n",
    "Results_df=PrintResults(results, 'Initialization',Results_df)\n",
    "Results_df=PrintResults(results, 'Normalisation_'+normalization_selected,Results_df)\n",
    "Results_df=PrintResults(results, 'Scaling',Results_df)\n",
    "Results_df=PrintResults(results, mode_KBest+'_KBest',Results_df)\n",
    "# Results_df=PrintResults(results, 'SelectFromModel',Results_df)\n",
    "# Results_df=PrintResults(results, 'RFE_FS',Results_df)\n",
    "Results_df=PrintResults(results, 'Multi-Objective_FS',Results_df)\n",
    "Results_df=PrintResults(results, 'Fuzzy_Chi_Classifier',Results_df)\n",
    "# Results_df=PrintResults(results, 'MPAES_RCS',Results_df)\n",
    "Results_df=PrintResults(results, 'Tuning_classifier',Results_df)\n",
    "\n",
    "Results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T13:38:43.430115Z",
     "start_time": "2021-03-08T13:38:43.423111Z"
    },
    "id": "tkzAC_kUY2ij"
   },
   "outputs": [],
   "source": [
    "## Save classifier\n",
    "# Results_df.to_csv(\"C:/Users/MASNA.CO/Dropbox/PAPERS/10-/Results/2Round_TPM_LR.csv\",index=False)\n",
    "# Results_df=pd.read_csv(\"C:/Users/MASNA.CO/Dropbox/PAPERS/10-/Results/2Round_ruvseq_diff_LR.csv\")\n",
    "# Results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLPWpXTAfWUE"
   },
   "source": [
    "# **Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T13:38:50.004044Z",
     "start_time": "2021-03-08T13:38:49.088870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAFiCAYAAADcEF7jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd5iU1fXHP4euICJFRUFR7F1ExWiiBoM1YsUWJHZj1yT23hJNosaa2NtPEVGjRhMl2KKxgZ2ggh1FQcVeIvr9/XHuuOMyszu7M7NT9nye53125r73ve99571nzy3nnGuSCIIgCILW0qHSFQiCIAhqm1AkQRAEQVGEIgmCIAiKIhRJEARBUBShSIIgCIKiCEUSBEEQFEXZFImZXWlms8zshay03mY2wcympb8LpXQzs/PNbLqZPWdmQ8pVryAIgqC0lHNEcjWwWaO0o4GJkpYFJqbvAJsDy6ZjX+CSMtYrCIIgKCFlUySSHgI+bJQ8Ergmfb4G2CYr/Vo5jwG9zKx/ueoWBEEQlI5ObXy/RSTNBJA008wWTumLA29l5ZuR0mY2LsDM9sVHLXTv3n2tFVZYobw1bkdMnjz5fUn9Kl2PIGhM3759NWjQoEpXo24otay3tSLJh+VIyxm7RdKlwKUAQ4cO1aRJk8pZr3aFmb1R6ToEQS4GDRpEyHrpKLWst7Uiec/M+qfRSH9gVkqfAQzMyjcAeKeN69Y0X30FN98M990HHTrAppvCtttC586VrlkQBEFFaWtFcgcwBvh9+nt7VvpBZjYWWBf4ODMFVhW88QaMGAEvv9yQduWVsPrqcM89sMgilatbEARBhSmn+e+NwKPA8mY2w8z2whXIz8xsGvCz9B3gbuBVYDpwGXBAuerVYiTYcccfKpEMzz4Lo0e3fZ2CIAiqiLKNSCTtkufU8Bx5BRxYrroUxWOPwZNP5j8/YQJMnQorrth2dQqCdshav722oHyT/7A7AG+eumpB+Zc48XkA1r9g/YLyP3LwIwXla0+EZ3tzPPNM83liETAIgnZMKJLm6NGj+TyHHAJHHQXTpjWkPfqoL8b37g0LLwx77AEvvli+egZBEFSIUCRN8emncOedzef76CM4+2xYbjnYaCM4+GDYYAP4299gzhyYPRuuvhrWXtunyoIgCOqIUCT5ePppWGstN/ltio03hp49G74/+CBceCF89928eT/7DPbayxfwgyAI6oRQJI2R4IILYNiwhqmq/v3h6KN9xJFh1VVh/Hj3K5k500ccG2zQfPn//W+sqQRBUFdUi2d72zNzJlx0Edx7r3/fZBPYdVc44QSfksqw2WZwzTW+znHmmfD22+6Q2L8/WHLIn39+GDPGj9/8Bv70p6bv/U51+VoGQRAUQ/tUJM8844rjgw8a0p58Es46q2FKqlMnVxy//rUrDnDFMWBA02UPG9b8/QcPbl29gyAIqpD2p0gk2G23HyqRDBklMmgQ3HhjYUqhMT//OSy6KLz7bu7zffrASiu1vNwgCIIqpf2tkTz8sK9TNMWf/9w6JQLQtSvccAN07577/AcfwKGHxoJ7EAR1Q/tTJK+80nye994r7h4bb+zhUw49FNZYA4YOhT33hPnm8/MXXginnlrcPYIgCKqE9je1NXdu83n6l2BPrcGD4bzzfpg2apRPfX3zDZx8sk9zHXRQ8fcKgiCoIO1nRPLNN/C73zX/j3vRRT3SbznYdFO49toGa6+DD/a1mCAIghqm/hTJZ5/BJ5/8MO2pp2CddeDYY+Hrrz3Ncuyl1bkzXH45dOlSvvrtvLObHWfYfXf45z/Ld78gCIIyUz+KZOJE+MlPYIEFYMEF3Sv9hhvgmGNciWSCL843H/zxjx6qZNQoWGgh6NULdtgB/vMf2HLL8tf1V79qWCOZOxe2287vHQRBUIPUxxrJrbf6niHZYUmeesrNfLPZeGO47LIGP46bbmq7Ojbm+OPh/ffh/PPhyy992mv11X2RvksX2Hpr96ZffvnS3E9yZTVpkluU/fznsSFXEAQlofZHJN9842sNuWJbZejZ0xXIxInV4wxoBuee26DsPvsMHnnE/374YUOQx8mTi7/XjBmw3noewuWww2CffWDgQDjxxDBDDoKgaGpfkTz0UPMhR049FfbeO/e6SCXp0MGn2TrkeQ2ffgr77VfcP/u5cz3My+OP/zD9m2/gtNM8rlgQBEER1L4imTOn0jUojr/9renR1OTJzTtQNsUdd8CUKfnPn3VWYSbRQRAEeah9RbLyys3nWWWV8tejtcycWZo8+XjggabPv/NO7v3ogyAICqT2FcmKK8LwebaB/+H5jTduu/q0lGWWaT7PH/4AL7zQ8rLnzGl6v/kMHTu2vOwgCIJE7SsS8DDvK6wwb/pii8Ett+Rfg6gGtt/et+NtinvvhdVWg9GjG0K8zJrlDpYjR7pvytixvu4BHuLl6KNhySWb35Fx8GBYdtninyMIgnZLfZj/Lr64m/veeCP84x/w7be+5e2YMe5TUs3MP78rgZEj3Qw4m3793Mdl2jRfcL/+es+71Va+oVa24+VNN3lcr7XXhuuug6++ajjXoUP+dZhTTqluRRsEQdVTP/9B5pvPAyPefLP7lRxySPUrkQw/+xk8/7yb5g4dCj/6kS+Cv/QSTJ3qYVWWWsrzzp3rC/SNvffBnS4vu6xBifTq5Sa+06b5yCfbaq1PH7j00nl9bYKgGcyso5k9bWZ/T9+XMrPHzWyamd1kZl1Setf0fXo6PyirjGNS+ktmtmllniQoFfUxIqkHBg92v5JcjB4NO+0EV1zhYV4++qjpsvr18w25fvWrhv3kx4+H11/3kVv37rDhhtCtW0kfIWg3HApMBVLj4izgXEljzewvwF7AJenvHEnLmNnOKd9OZrYSsDOwMrAY8C8zW07St239IEFpqJ8RSb3TpYsrhjFjms87bhwcdVSDEskwaJCHY9l001AiQaswswHAlsDl6bsBPwXGpyzXANukzyPTd9L54Sn/SGCspK8lvQZMB9ZpmycIykEoklqjkLAmiy1W/noE7ZXzgCOBzKJbH+AjSRlnpBnA4unz4sBbAOn8xyn/9+k5rvkeM9vXzCaZ2aTZs2eX+jmCEhKKpNbYaaemPfSHDAkrrKAsmNlWwCxJ2XF7cjVGNXOuqWsaEqRLJQ2VNLRfv34trm/QdoQiqTWWXtojGueiWzcPAlltoWCCemF9YGszex0Yi09pnQf0MrPMeusAIBOzaAYwECCdXxD4MDs9xzVBDRKKpBY5/XTfNyXjO2MGW2wB//43rL9+ZesW1C2SjpE0QNIgfLH8Pkm7AfcDO6RsY4Db0+c70nfS+fskKaXvnKy6lgKWBZ5oo8cIykBYbdUiZrDXXm7u/MknvhCf2Q8+CNqeo4CxZnY68DRwRUq/ArjOzKbjI5GdASRNMbNxwH+BucCBYbFV24QiqWXMasdXJqgrJD0APJA+v0oOqytJXwE75rn+DOCM8tUwaEtiaisIgiAoilAkQRAEQVGEIgmCIAiKIhRJEARBUBQVUSRmdriZTTGzF8zsRjPrli/wWxAEQVDdtLkiMbPFgUOAoZJWATriZoGZwG/LAnPwgG9BEARBlVOpqa1OwHzJ23V+YCb5A78FQRAEVUybKxJJbwN/BN7EFcjHwGTyB377ARHILQiCoLqoxNTWQngY6aXwvQi6A5vnyDpPEDeIQG5BEATVRiWmtjYBXpM0W9I3wK3Aj8gf+C0IgiCoYiqhSN4EhpnZ/GmTm+F4zJ18gd+CIAiCKqYSaySP44vqTwHPpzpcigd+OyIFeOtDQ+C3IAiCoIqpSNBGSScBJzVKzhn4LQiCIKhuwrM9CIIgKIpQJEEQBEFRhCIJgiAIiiIUSRAEQVAUoUiCIAiCoghFEgRBEBRFKJIgCIKgKEKRBEEQBEURiiQIgiAoilAkQRAEQVGEIgmCIAiKIhRJEARBUBShSIIgCIKiCEUSBEEQFEUokiAIgqAoQpEEQRAERRGKJAiCICiKUCRBEARBUYQiCYIgCIoiFEkQBEFQFKFIgiAIgqIIRRIEQRAURSiSIAiCoChCkQRBUBBm1s3MnjCzZ81sipmdktKXMrPHzWyamd1kZl1Setf0fXo6PyirrGNS+ktmtmllnigoFaFIgiAolK+Bn0paHVgD2MzMhgFnAedKWhaYA+yV8u8FzJG0DHBuyoeZrQTsDKwMbAZcbGYd2/RJgpISiiQIgoKQ81n62jkdAn4KjE/p1wDbpM8j03fS+eFmZil9rKSvJb0GTAfWaYNHCMpEKJIgCArGzDqa2TPALGAC8ArwkaS5KcsMYPH0eXHgLYB0/mOgT3Z6jmuy77WvmU0ys0mzZ88ux+MEJSIUSRAEBSPpW0lrAAPwUcSKubKlv5bnXL70xve6VNJQSUP79evX2ioHbUAokiAIWoykj4AHgGFALzPrlE4NAN5Jn2cAAwHS+QWBD7PTc1wT1CChSIIgKAgz62dmvdLn+YBNgKnA/cAOKdsY4Pb0+Y70nXT+PklK6Tsnq66lgGWBJ9rmKYJy0Kn5LEEQBAD0B65JFlYdgHGS/m5m/wXGmtnpwNPAFSn/FcB1ZjYdH4nsDCBpipmNA/4LzAUOlPRtGz9LUEJCkQRBUBCSngPWzJH+KjmsriR9BeyYp6wzgDNKXcegMsTUVhAEQVAUoUiCIAiCoqiIIjGzXmY23sxeNLOpZraemfU2swkpzMIEM1uoEnULgnrHzHY0swXS5+PN7FYzG1LpegW1S6VGJH8G/ilpBWB13PLjaGBiCrMwMX0PgqD0nCDpUzPbANgU9z6/pMJ1CmqYNlckZtYT+AnJskPS/5JNenY4hewwC0EQlJaMhdSWwCWSbge6VLA+QY1TiRHJ0sBs4Coze9rMLjez7sAikmYCpL8L57o4wiYEQdG8bWZ/BUYBd5tZV2K9NCiCSjSeTsAQvCe0JvA5LZjGirAJQVA0o4B7gM3SbEBv4LeVrVJQy+RVJGb2OzPbL0f64WZWjP33DGCGpMfT9/G4YnnPzPqne/THg8IFQVBiJH2By9cGKWkuMK1yNQpqnaZGJCOBy3Kkn5/OtQpJ7wJvmdnyKWk47uGaHU4hO8xCEAQlxMxOAo4CjklJnYHrK1ejoNZpyrP9O0nfNU6U9G3aU6AYDgb+L+2k9iqwBynkgpntBbxJHo/YIAiKZlvcQ/0pAEnvZMyBg6A1NKVIvjKzwZJeyU40s8HAV8XcVNIzwNAcp4YXU24QBAXxP0kyMwEkY5cgaDVNTW2dhFt0/MLMVkzHaOCudC4IgtpkXLLa6mVm+wD/Ivc0dhAURN4RiaS7zGwGcCQNFh0vADunEUUQBDWIpD+a2c+AT4DlgRMlTahwtYIapsnov5KeBXZro7oEQVBmUgj4eyRtgm+VGwRFk1eRmNlt/HD7SwHvA/dLGlvuigVBUHqSscwXZragpI8rXZ+gPmhqRHJhjrTewG5mtpqkY8tUpyAIystXwPNmNgF3CAZA0iGVq1JQyzS1RjIxV3oaqUwCQpEEQW1yVzqCoCS0eIdESXOLdyMJgqBSSLqm+VxBUDhNrZH0zJG8ELA7HvY9CIIaxMxe44frnwBIWroC1QnqgKZGJFPwxpYZfmQW2x8E9i9zvYIgKB/ZzsDd8CgSvStUl6AOaGqNZGC+c8mEMAiCGkTSB42SzjOzh4ETK1GfoPZp0RqJmf0E2BXfdGrRstQoCIKy0mhb3Q74CCVibQWtpllFYmZr4cpje6AfcAhwfJnrFQRB+fhT1ue5wGv4HiVB0CqaWmw/BdgJeA+4EVgbeELSFW1UtyAIysNekl7NTjCzpSpVmaD2aSpo40H45jfnAldKmk0OS48gCGqO8QWmBUFBNDW1tSiwGbALcGHygp3PzDrk2qckCILqxsxWAFYGFjSz7bJO9cStt4KgVTRltfUNcCdwp5nND2yNmwi+bWYTJO3eRnUMgqA0LA9sBfQCfp6V/imwT0VqFNQFBVltpT2exwJjzWwhYLtmLgmCoMqQdDtwu5mtJ+nRStcnqB9aEyJlDhAL7kFQuzxtZgfi01zfT2lJ2rNyVQpqmaYW24MgqE+uw9dAN8UjVQzAp7eCoFU0q0jMbJ5RS660IAhqhmUknQB8ngI4bgmsWuE6BTVMISOSJwpMC4KgNvgm/f3IzFYBFgQGVa46Qa3TlEPiwkB/3OR3VRqCN/YE5m+DugVBUB4uTUYzJwB3AD2IOFtBETQ1RbUlsCc+f3oRDYrkU7wBBkFQg0i6PH18EIjQ8UHRNOVHchVwlZmNkjSuDesUBEEZMbNFgDOBxSRtbmYrAetF+KOgtRSyRrJwZpMrM/uLmT1hZsPLXK8gCMrH1cA9wGLp+8vAYc1dZGYDzex+M5tqZlPM7NCU3tvMJpjZtPR3oZRuZna+mU03s+eyow6b2ZiUf5qZjSn5EwZtSiGKZF9Jn5jZCHya61fA2eWtVhAEZaRvmmX4Dnz7bODbAq6bC/xa0orAMODANJo5GpgoaVlgYvoOsDmwbDr2BS4BVzzAScC6wDrASRnlE9QmhSiSTKDGzYGrJE0u8LogCKqTz82sD0m2zWwY8HFzF0maKemp9PlTfMvtxYGRQGYf+Gvw/YpI6dfKeQzoZWb9cf+VCZI+TA7OE/C4fkGNUog/yLNmdjewHHCcmfUgogAHQS1zBG6tNdjMHsH3GdqhJQWY2SBgTeBxYBFJM8GVTbL4BFcyb2VdNiOl5UtvfI998ZEMSyyxBH1bUsGgTSlEkewBrAVMl/SFmfUF9ipvtYIgKBeSnjKzDfEgjga8lIK0FkTqTN4CHJamvfNmzXX7JtIb1/NS4FKAoUOHKnqv1UuzU1SSvsVNBH+VkuYr5LogCKoLMzsz6+vGkqZIeqGFSqQzrkT+T9KtKfm9NGVF+jsrpc8ABmZdPgB4p4n0oEYpJETKhcDGwC9S0ufAX8pZqSAIykL2OsRZLb3YfOhxBTBV0jlZp+4AMpZXY4Dbs9J3T9Zbw4CP0xTYPcAIM1soLbKPSGlBjVLI1NaPJA0xs6cBJH1oZl3KXK8gCKqP9YHRwPNm9kxKOxb4PTDOzPYC3gR2TOfuBrYApgNf4NPkmf8hpwFPpnynSvqwbR4hKAeFKJJvzKwDDRYefUhmg0EQ1BQLm9kR+BpF5vP3NBplzIOkh8m9vgEwj2+ZJAEH5inrSuDKQiodVD9NxdrqlOzLL8LnRPuZ2SnAKOCUNqpfEASl4zJggRyfg6AomhqRPAEMkXStmU0GNsF7IztKeqFNahcEQcmQFB3AoCw0pUi+H8JKmgJMKeWNzawjMAl4W9JWZrYUvp1vb+ApYLSk/5XynkEQBEHpaUqR9Gs8h5pNc/OpBXAo7hnbM30/CzhX0lgz+wvuq3JJkfcIgiAIykxT5r8d8X0KFshztBozG4CHqb88fTfgp8D4lCU7zEIQBEFQxTQ1Ipkp6dQy3fc84EgaFFIf4KO0uA95QibAvGETgiBoGWZ2vKTT0+eukr6udJ2C2qapEUneuAfFYGZbAbNS8Mem7pUzIoKkSyUNlTS0X79+5ahiENQlZnakma3HD+NqPVqp+gT1Q1MjknLtObI+sLWZbQF0w9dIzsMjg2ZMjiNkQhCUnpdwZ8Glzezf+BplHzNbXtJLla1aUMvkHZGUy9NU0jGSBkgaBOwM3CdpN+B+GnpK2WEWgiAoDXNwT/TpwEbA+Sn9aDP7T6UqFdQ+1RR88SjgCDObjq+ZxLafQVBaNgPuAgYD5+CbSn0uaQ9JP6pozYKappAQKWVD0gPAA+nzq3jDDoKgDEg6FsDMngWux/cT6WdmDwNzJP28kvULapeKKpIgCCrCPZKeBJ40s19J2iDtMxQEraKapraCIGgDJB2Z9fWXKe39ytQmqAdCkQRBO0bSs5WuQ1D7hCIJgiAIiiIUSRAEQVAUoUiCIAiCoghFEgRBEBRFKJIgCIKgKEKRBEEQBEURiiQIgiAoilAkQRAEQVGEIgmCIAiKIhRJEARBUBShSIIgCIKiCEUSBEEQFEUokiAIgqAoQpEEQRAERRGKJAiCICiKUCRBEARBUYQiCYIgCIoiFEkQBEFQFKFIgiAIgqIIRRIEQRAURSiSIAiCoChCkQRBUBBmdqWZzTKzF7LSepvZBDOblv4ulNLNzM43s+lm9pyZDcm6ZkzKP83MxlTiWYLSEookCIJCuRrYrFHa0cBEScsCE9N3gM2BZdOxL3AJuOIBTgLWBdYBTsoon6B2CUUSBEFBSHoI+LBR8kjgmvT5GmCbrPRr5TwG9DKz/sCmwARJH0qaA0xgXuUU1BihSIIgKIZFJM0ESH8XTumLA29l5ZuR0vKlz4OZ7Wtmk8xs0uzZs0te8aB0hCIJgqAcWI40NZE+b6J0qaShkob269evpJULSksokiAIiuG9NGVF+jsrpc8ABmblGwC800R6UMOEIgmCoBjuADKWV2OA27PSd0/WW8OAj9PU1z3ACDNbKC2yj0hpQQ3TqdIVCIKgNjCzG4GNgL5mNgO3vvo9MM7M9gLeBHZM2e8GtgCmA18AewBI+tDMTgOeTPlOldR4AT+oMUKRBEFQEJJ2yXNqeI68Ag7MU86VwJUlrFpQYWJqKwiCICiKNlckZjbQzO43s6lmNsXMDk3pOT1kgyAIguqmEiOSucCvJa0IDAMONLOVyO8hGwRBEFQxba5IJM2U9FT6/CkwFXdIyuchGwRBEFQxFV0jMbNBwJrA4+T3kG18TXi7BkEQVBEVUyRm1gO4BThM0ieFXhferkEQBNVFRRSJmXXGlcj/Sbo1JefzkA2CIAiqmEpYbRlwBTBV0jlZp/J5yAZBEARVTCUcEtcHRgPPm9kzKe1Y8nvIBkEQBFVMmysSSQ+TOwIo5PCQDYIgCKqb8GwPgiAIiiIUSRAEQVAUoUiCIAiCoghFEgRBEBRFKJIgCIKgKEKRBEEQBEURiiQIgiAoitghMQiCoBU8+JMNC8q34UMPlrkmlSdGJEEQBEFRhCIJgiAIiiIUSRAEQVAUoUiCIAiCoghFEgRBEBRFKJIgCIKgKEKRBEEQBEURiiQIgiAoilAkQRAEQVGEIgmCIAiKIhRJEARBUBShSIIgCIKiCEUSBEEQFEUokiAIgqAoQpEEQRAERRGKJAiCICiKUCRBEARBUYQiCYIgCIoiFEkQBBXBzDYzs5fMbLqZHV3p+gStJxRJEARtjpl1BC4CNgdWAnYxs5UqW6ugtXSqdAWCIGiXrANMl/QqgJmNBUYC/61orcrIhb++s6B8B/3p5wCc8YsdCsp/3PXjAZh6xn0F5V/xuJ8WlK8lmKSSF9pWDB06VJMmTap0NeoGM5ssaWil6xHUP2a2A7CZpL3T99HAupIOysqzL7AvwBJLLLHWG2+8UZG61iOllvWY2gqCoBJYjrQf9GolXSppqKSh/fr1a6NqBa0hFEkQBJVgBjAw6/sA4J0K1SUoklAkQRBUgieBZc1sKTPrAuwM3FHhOgWtJBbbgyBocyTNNbODgHuAjsCVkqZUuFpBKwlFEgRBRZB0N3B3pesRFE9VTW2Fg1IQBEHtUTWKJByUgiAIapOqUSRkOShJ+h+QcVAKgiAIqphqWiNZHHgr6/sMYN3GmbKdlIDPzOylHGX1Bd5vwb0jv7NkC8oIgjZj8uTJ75tZLo/EWpGtastfUlmvJkXSrIMSuJMScGmTBZlNaonXZuQPgupGUk6PxGqTlVrP31qqaWorHJSCIAhqkGpSJOGgFARBUINUzdRWiR2Umpz6ivwtzh8E1Uq1yUqt528VNR39NwiCIKg81TS1FQRBENQgoUiCJjGzaCNB0E5orbzHP4lgHszse1NsSd9Vsi5BEJSXUsh7u1Mk2T9atWFm3VuYv0Pj57FEMfVQWjgzs/vNrG8xZQVBJalWea8WWYfSyHu7UyQq0rrAzFYzsx+b2QAz28rMNjSz3kWU1yX9HQBkth0ttHFsA+xoZgtmEpRo5p4d0t9hZrZCnnNrAK9Lej9Tp5jmCmqNYuS93mW90fmi5L1qzH/LiZl1lPRt+rE2AF4DXgXeBj5tYWO7FA87cDmwRfp8r5k9h/vCvCrp8xaUt6yZrQiMAd6AH/QQegJdMi83BycAB0j6OOXfChgG3CZpcr4bZg1fjwWOzHPuJ8AIMzsAuFbSZ+SINBAE1UYJ5b2uZb3R+aLkvV30MCV9mz5eAvwYOBQ4Dfg1sG22li+APwJz8MbQH/gYj1Y8BrgM2NvMFm5BeR2BlYHNgE5mtreZDU/njsMFYR7MbF3gc0mPmlk3M9sf+B2wNHCrmW2Y57pMD2RR4AWge9a57N7RJOBKYGPgOjM7w8xGtOC5gqAilFDe24usQ5HyXvd+JGZmkmRmawLHS9o+pa+H9zKGATtL+qDQ8oDheC/lJuBOPLjkhngjex74HDhY0pwCy+yLD12/AVYB5ge+AnYC1pY0M8c1uwHrSDrUzLYB9gRulXS1mY0CRkjau4l77gdcCDwGnArcB3yX1UPqBHTFw9Ysgf9OKwG/kDS3kOcKgramlPJez7KeRmxWMnmXVNcHDcpyH+AR4EeNzndqRVkdgQuAqcDJQBdgIeDfwArAOGCDAsrrmP7uByybPvcAVscb/ZrZ92107QLAM/gQ+yXgCGD+dO4k4A/Z98hz/37AAcDTqawrgIXx+dtxuHJZOateC+erTxxxVMNRKnlvJ7K+SDpXtLzX/Ygkg5ntAWwN/A+fn3wOeFzStGzN3MT1ywK7AL2Bmek4HlgWmAjMxn/44Wb2IrCWCpg/NbOuwKPASElvmdmZwLvAZZK+zHPNUsAXeHjonYCvJd2SznXDBWhPSc826nV0kPSdmfXBw0ivB9yR7rsmsD8uNDcAW+E9mNXSvXYAbs5XpyCoJoqR93Yk68cDnYF/UqS8txtFAt+/yI3xBrE8PpQ7QNI3BVx7It4juQ//8f+GzyuOAt7DIxVPxHsY20rao5nyMkPw0cDGkvY03154fdwIYoKkc/Jcexlwt6TbzKxzpv7p+bYDtpO0Y47rMo3rfHw4/SN8iHyOmfWXNDMJ4ADgZuCPkrZKi5bXSFq9ud8pCKqF1sp7O5D1RYFZ6Xxp5L1cQ8xqOIAO6e+auAXGWcDPgJ6kefi1QFEAACAASURBVMAWlLU4sD1wCvAs8BmuybcHemQ6Arhlx8AWlLspPvd6BXBuStsauDpTZqP8PYCnaBgqXwQs2ijPgtnP3+hcZ3zhrQPeO1o9pf8VGIpbbxwHPASMTueOB85TM8PnOOKo5FEqeW8vsp4+l0Te69pqSw1Du+vxH/JbfH7xHnyI+Fgh5Zjbfy+Av9TlUlmnAQviliGvm9mRct6X9Fb+0uZhIr6t8MfAGSntN8D/ZW7fKP+uwGdqMG8cKuldy3JYUjIRVG4v1WXSPZdO5TxrZivjC4jPS3oo1WVBYCkzux7vzVySrm8/Q9igpiiFvLcDWe+JGxC8mK4ribzXrR9JZmiHa/zHJF2Vde5kvHdxcYHFnYpr7r7AfMBBkm43sxvwBbfh+CIWZtZJzVg5ZA07F0zXfw4cI+lrM1sSeE3SBMjZQPoBs8y3HN4JeDxPvsb3zMyfvgR8AjwB/DMJ3hm4rf1mKduFZnYvMAK4DZgs6fVC7hMElaCE8l7vsr4PbtK8vJkNBL4thbzX/RqJmW0H7I7PeU6T9LmZHQGsIGnfJi/263sAj0pa1czWwucSO+I9nWclTbfkANWKut2JDykPBn4JPAj0ljQ7nZ9nUdDMFgHWxk0HV8V7E9Nwh6v/SpqW4z5LAgMkPZJp/Ga2V7p+C9wC5fe4yd+v0mV3AuMkvZLK6BBKJKh2ipH3diDrmwPjgacljTez3fFFdyhS3utSkZhZb0kfZn0/FbePfhQYhE/jHCTp6QLK2h93aFoFd+jZAvgDrrm/wi04Jkp6qsC6Zdu5/x4fvv4LX58A7zUdq8Ls3Afhw9DlgD74XO5xjRuAua15prxdcJv4V4Cv0/2HAAcBawH3pzK3xxXLt8BwSV8U8nxB0NaUSt7bgay/g0+llVze63Vq62Azuwe30pgj6UQz2wxvUO/hjkrNKpGE8B/3XNy7tTP+ci7ErS5+gs9DFlZYg+ZeAbgVf5mPp3nQH+H25HkbVtbcqNLw8/U0r7sm0D1PL2I5vCGvDHyKN6bPcEEbCLyOL+Q9KelevLf0YBoKryzpixiRBFVMqeS93mX98ZRnOqWWd1WBtUWpD6Br+vsb3JHobmA3YIGsPM062eBzlIekBrAfbsHxPq7hLwY2KaScPGUvgJsVfgb8KqVdBxyhAq0l8MW5XA5My9BgwTICd2RaFDgjK8+W+ILaM8A1wJfA+biJ4xC81xOOh3FU/VEKeW8nsj4Zn+K6qNTyXpdTW/D9UHB+3OFnE9zBZgW8hzJSBQzdzOwEfI7yADzgWz+8l7Ml3itYBPiNpOsLrFNH/GXNTd8HAPsC6wCDgauBiyR9lGvOtJmyv+9BmNmNwG7yRb6VgWPwYessPDzEmynfonjjXhy3SZ+Vng988X2SpDsLrUMQVIpi5b3eZT3lXVrSq2a2PCWW97pTJGa2lKTXzL1Gn5R0W/Y5YDlJ9xRY1mO41UNfYAYeRmA8HidnaXxucSLe8EzNW1MciTfM2Xjv5z3cUWoW3it5rwXP+YNFvyzrkK2ADSX9NutcP9zS5F6gV6r/Q7ij1ol4TJ3PJL1gHnNnLbyhzZB0QUsbehC0FaWS93Yg6/fiaybn49NfX5RS3utKkaRewGh8iDsQ2FvSzVnn98G9SF8voKy18TnFy/AFqV7ASNwL9EngRiWzvQLrtgy+aDcK7/V0xZ2kXsFNAP9YYDnzA98oyzs3zaV2kM+9XgucKrcw2QQPd/AYHldnZuqNDMNDJqyBRyK9AjcJnIp78D4j6eOsBhuKJKg6SiXv7UTWl8FD0T+BK8mSyntdKZIMydxtf3zapice6uB2fNHsRwVOa12IL1bNxs31DPgH/qJWBn6BD03PL7BOJwILSTrczLbGfTd2xUcD++MhCa7Oc21mf4Wd8eH6/sAhksY1ytcDeEjSkPT9Z7hn67Z4iOqbgH/JTSI7AUtLetncxn04HnJ7sfTcD0m6tpBnC4JKUqy8tzNZt/QblVbeW7u4Us0H7kiUCSuwEb649B/gxBaU8XD6wXvic67X43bl/8UdfTYC5lMBC3lZ5R2LB4K7hRSOIJ07DPhzc2XhC+PL434fG6a0/YEl0uefArtk5e+U/q6V7nEt8Hd8L4Ph6dxRZIV5wOeZ9wBGFfpsccRRyaNYeW8Psk7DoKEs8l7xRlCGRtUZX9A6Glit8bkCy+gFbJU+/wUfEj6CmwbOxC0whrSgYXXG7bavBK5KjXN7YPl0/p9Z/9jniZmT0jfEexk98LngTPokoE/63DPzjFmCtWj6Ox/e+/hpep6b8cXFx9P5JfAprqtCecRRK0ex8t4OZP34JOudcOusssh73UxtZQ0Jf43PB66Om7q9hMfauUrS9FaUOwkfKo7FLSJ+BvwJOFTSxYXYW5vZGNyO/Ft8Y5z18QWvj/Bh5UhJqzRxveEN45e4hdWrkv6Yhr+jJW3ZuB5Z853/AK6Qe7IuAczFpwA64z2X9/EpgHPwkNtf4R7BMa0VVC3lkPc6lfVPcfPo983sNMok7/XkkJj5YbfAQyQcgf9gnXEb6k9xR52CMbPV8SFmN2BJSVPN7GX8RV+VshWiibcCPpJ0O76Yd7+ZLYZbgozALSnmsc7IINf2b5vZLLy3856ZLYTvHXBlprqNrvku3WNR4BYzWw3vmXwLnCxpnJm9jscm2g+fA77IzDLOWBEWJahmSirv9S7rKevrlEveKzksLfWRfsi78fAGTwJdUvol+GJTa8rsh++IdnF6sb/H9wcAj0vVpYAytgf+nT53anRufZqZfwV2xE0PF8KHrXukejQZwho3P7wYn2u9DrfQ2gDf4CbTIH8G7JW+dwdepmHntJjiiqNqj1LLez3Leuae5ZL3upjaypirme8Y1gXvOfwOjy/zHrC9pHVbWGZn3HTuP/Ih9FZ4I3kXt+h4A7hY0pYFlncXcK2km8ysP+4wNQJ/ies09Wx4AzgBNyP8gwrfH7obvtjXF7hEvsfzEbhd/Ct4g5+U8vZJ+baUb34To5GgKim1vNexrB8OLCPpwCT3DyjFCSu5vFe6V1HKAx82Zqwa1sDnN88EdmpBGZmFq32Bv6XPHdKxYNb544BtmimrO25PviW+qc21+B7LU/Few/Y0WFs0GSYhvfQzcQ/UfcmzkEiDdUZfUq8MGJyV9ijuqfsKHrqhBx6Y7hJglUq/wzjiKPQoVt7biayvgiulV5Ksl0XeK94YStCYMnFm1sfnTW8HuqW0XnhI5ZaW1RU3uzsyNarMrmijgU3T53+ThqlNlLcAbjVxNx7++Tt8CN6zgLpkGl3GSmNgqtfa6Rl3yHNdpvGfCvw2fV6CFCsoKZGdca/dHql+twGHA+dX+n3GEUdTR6nkvd5lPX1eM/0tu7zXww6JmYWnMcDeeLC1X6a0ITTsr9ESrsY1+kjcAuQY82id+wL/Swtzb0r6sqlCJH0q6fRUzijcRPEL4HYzO8c8/k6+azMb5lxjHr7hKNwr9Ry8l/ET8AWyRtdlFvB2Bi4xs+WAP+K28efJo6BOw8NA3An0krQtHnp6gVxlBkEVUWp5v5o6lHUz+7MaIh6XX94r3cMoVS8Ft7Hui5sB3pbSLwf2a0E5O+DOO8/i0UPfwe25b8RDL/835RuCW3a0pq498AWvscBm2T2LrDxr4NYfHXCHoUXxcA2Wzq1Ew6Jdrr2aB+OxdYbi+zNnIo4+QsMezyNwC46+6fsDwIh8ZcYRR7UcpZD39iTr6XtZ5b3ijaLIBpWZF+yReVHp++/Sy3sKmL/Asjrimjuzu5rhO4o9gO9M9ldgrVyNoQzPtQGwZHr5N+C9r1VbcH0X4Lf4HOvpKW0bfOGwCz4tsEJW/sWAwyv9PuOIo6mjVPLeDmR9JPCP9LkrbnpcVnmvaastM7sEj167ITBd0jMp3fA4Oe/Ih3GFlLUzvi+y8PWEKfh845X4wtuZkt4tYd0zTkSD8TneKY3OZ0z1BuFWVr3xRv4acIOkj1p4vxtxRbI1vlXnobhVyg3AhZLeLu6JgqC8lEre24usS7rWzMbjC+2HUUZ5r2lFAt9HAL0IX5j6Ag9BcCUeErqPsuLxN1POScBcSWekoGu74o3qNbzHcrek00pZb7mp4RXA5ZIeTenzSfrSzFYA1pV0TWqAg3EP2UHAKZI+bVReprGugg+Ve+C9rpfTM4APm8/BrUoexs0ML8Yb71KS3ijV8wVBOSiFvNe5rL+KT5O9i0/7tYm816xnu5ltAzwv36x+/2RHvQUe/fJB4BFJexRYVgfc2uJ8M1sXX9w6E7gAd+w5Hfgwk1cl8K9Qw0LZKvjCWobVzGwdfO+A21PeV4BXzOw1oHfjhtWIP6Xr9sDt379Lf6/GG9T49PcVSfcm+/IlQ4kE1Uyp5L0dyPrrwHXyMPLb0EbyXsvWOVsDz5vZs2Z2KL5gdKuk0cDGNIQ1aBZJ30l6Et9+cgA+TF4Hj7fzID4Evik5QhXcsMxsSTNbz8zWNLOuWemd0t+f40PQb9N3w4e0PfE4PSuY2dFmNiJduh8+v5vzGcx3POwp6WK8h7Y/cBceEfQDvOd2K27z/mWy8tgVD6Xw/R7RQVCFlETe24GsD8XD4UMbyns9TG3tAhyIx6KZhHugjm/B9Zlh4hb4HsqbmFkvfM70RLx30knSnoX0ULLKWxVvnIPxzWk2MLPukj7Pynsnvug1vVEZi+EOTNPwRrYoHjJhfXzB8RtyYGYb40rjLuBcSZul+eA/AbtKejAr759TmX2AX0qanYSnthtEUNcUI+/tQNa74tZar6Tfpc3kvSYVSRqedlCD/TVmtgCwD76I/L6ktVpY5k7AMEmHZ6XthS/sHS7pgxY2rovwHs7nwI6Sfmlmo4FlJZ2YnmEnSTfmuPYw4Hp5xM5F8AYKvj3mM43rYW6j/r6kr8ysC25GeDXuoPUd3iP5Ag9Idx9wtaRpZtZPUqb3EgRVSanlvY5l/VN8K99ZeMiXtpN3VYFZX2sPXCOfjls8dMlK71Xg9R1p8CpdAN/r+E58R7Ql8Y1h9sjkbWHdrsaHzHcAG6e0y/GeEPi0omXlzyj1/vii2XyNyssX5G1d3IP2NHz/gX4pfWF8uHsQsAiuTIbjYSUm4XOrv8UbWwRnjKPqj2LkvT3JekprU3mveONoRWPKhDbYJL24y/E5v6nphf6oBWWNAFakYXe0bqlhXYfvjnZgSxtVVtlr4xvHTMbtttcGngMWzpM/07gOBi5LnzOhH9YAbmniXmumxjURD3/wG9yRqlfW9Stm3WNhfIHvutY+XxxxtMVRKnlvB7Lek4aQKW0u7zVrtYX/MH/FzfbexucK/4Zbov2nwDJ6pbz74PFtHsYb6G3Kmt8slMyco5mth2+w8wDe2zkt1fNiSbNyDZuV3jge1+fHZra8pJdS2ja4Wd8P9jFIc6KjgJclnZAW9jbDLVk2w0M8HA38Gd/ec2oqbzYwVmmoHWsjQQ1QrLzXvazjYVmeowLyXouKRMnaYGHgceAy4BxJT5jZ1cCEggvyzZ064mESuuMeoV8Cr5rZS8C9aibGTqPylMr7FTAHj/55J947eC6rwTb1EqfiDekSM3sP3+5zTbzHBA0b+gDshXut3p/uPxcfov/dzHrgjexL3Lrj1vS7dZL0jZmdZmYXSJoVSiSoYkoi7+1E1l83D1vf5vJeU4vt2ZrUzLrjQ9wj8M1cHsGHeasX0iDMrKekT8zsQOANSX83swVxu/KN8aHmAa2s5yA89LSAM9SMzXaj51oC7z0ZPmRdArhU0swc192fyv9XWtBTauDL4POpY/Fe0inAbyS9l64bDNwpaaXWPF8QtAWlkvf2Iutyp8elqYS8l2O+rFwH8C/chG2drLRu+ALcH4HDWlDWJnhcmxdoFKYZD1GwaPrcooBm/HBR7Uh8kWuLQsrB5zDPwp2Wlm5cXqO8iwKTc6Rn5knH4pYp4N6tr6TfaQvc5v7M7PxxxFFtR6nkvT3Jevre5vJe8cbSwhfXM/2dhm9ifxWwUivK6YhbWYzFnXl+D+yJDx0XwfduHtSKcnvgW2WehId2Ho7Pg36MhyTIdU1mMXHLVJ/1M40Gn8s9ghwb2+CLa9eSts1MaZ2zzr2QfV16tkvweeJRuNdsKJI4qvYohby3R1lP6W0q7zUztZVld720pFfN48wchM8NfgVcKemUFpa5OG7h0B03KeyG24IvJWnTli5Kme9jsDfwIm6eNwUPWfBjSaOaufYMfP6zLz5cP8bMdgN+IWnzXHUx94I9FrhC0nUprSfuFTtU0k7ZC3ZBUCuUWt7bi6wD37XkOUpGpXsdLegBZHondwBrNDq3CTC6heUZ0BlvXBltvTwe6CwTs7+goS7eE+kErNgoff7G92yijNXwofF7wHop7XbcIx3y9CTwUNiTcCekO3FrlstI22hmPwMNQ+HM31VIw/o44qimo5Ty3h5lPbuctpD3mhiRmFlvfJ/hn+I/yhKNzm+DLyY12/PO6un8GO/hfI3PQU7Dg7m9oBYGNEsLd//DN8f5PJVzvqQXmrluOeBtJQsPM9sa3+KzEz7UfRI4qMDnWhofXj8HTJIvvOXsZWWZLv4d2E3Sxy143CAoK6WS9/Yk6ym9YvJeE4okg5ldjs/9LY7/8Gfj8XEulTSkwDIyjWscHpt/I9zTdTYeGO4G+ZaZhdYpU95ykl42syG4+d6WKcvJkv6S59rz8T2Ut8JN/17AHZq+w4fek1IDyDvszrbgyHEu20LEsssys6WAv0oa0fi6IKgGipX39irr2d/bTN7LMcwp5YHvADYqfc5sZt8Vd/d/BXgU17ItLfP+9Pkh0u5h+L7mq6vAoS5pe8z0+UEaebLiu65toaaHqx1wR6v7gHH4nOeqQNcWPpPR0DHohIeWPjxHnTJhIk4Etqr0+40jjuyj1PLeDmS9A25QcCHuUb9Ao7xtIu8VbzgF/GiD8VAGA/CQCLsB/bPOfx8aoAVl9sS9QXviliArpxfyYuaHL7CcgXiYghnAlBznt22qYTb63hG3af8r7iV7ZXMvPtOYGpeJb9/5FPAR3gvaMf2O2VZcD5PCMsQRR7UcpZb3epX17HLx8C+f4Xu3V0Teq96zXQ0bvfQBpuNWG3ua2bP4S3hQLbdK6irpn8nz8wHc2/VRPFzC3EItnSS9BWxrZrcBK5rZR6mcM/C52H2A25oZro4EVsIDuP0HHyovgtt/n2EejvqmlLcLHlvogXT/HwxlcS/gjngAt5fx0A3f4ttsTscX58alOehbJH1V8C8WBG1AGeS9LmU9fc/sJ3IKvo32A7jCbHt5r0SvoyUHuTXxinj8/6eATVpSDv7SLyUNAWkYrg4iDTFz3TNPmZkeQcahqFeq12v48PWnKT3fUHcI8Dxui/4L3O777Kzz5wMjsr6vg8/tLoKbAg6nkRVGOvcCcA++9Sj4Dml/wxvv4S15xjjiaMujFPLejmTd0vlPM7Ke0ttc3qt6sd0a9jpeF/dM/Rz/JzlJ0pSUpyD776yyTga6S/qtme2Kz73OAHaR9FkL6pZZeOuLmxV2xq1A3krne+QrL1md3I3vVtZV0l/N4+X0xx2m/iXpkhzXdcKH5euka7/Bh7Sv4COQZ/CAcdfiQ/ERkl43s/mBW3BHrGuArRWjkaDKKJW8txdZl/SZma2NjzwuBK6tlLxXuyLJNlt7Efca7YcvoH2GD9cebWGZtwEX45YSW+M/9A7AQ5JubUXdxuONc2/cxvs1fDQwUTk2kUkN5AZ8iAveS9kn0xDN7BLgRUl/tiY21zGPBro8vh6yNB7U7mxJLyShGYk7Sa2Qzt2evh8u6eeFPmcQtBWllvd2Iuur4wv2w3D/ksrIezmHO6U48B72zVnfF8Adkn4PrFxgGdkxcXbCY/m/nrkeHwKu0jhvc+Xhvf7H0+dJuMfs3XhEz2WaKWMJvNc1CZ/PvAmf67yEhn1EsuudcSpaCh+6HoA3no7AfHgj60jDdNaKuJ36bvhiYzd8UW+zSr/TOOLIdxQr7+1I1q2a5L1qRyRZw8nN8J7E9bjjT4sciFJZu+Mesl/idtuL4BYNr6XyT5Y0rBXl7gksg89HnixpCzMbCJwuaUwT1w3Gh6rdgQ/wHsRovLH9XdLe+YbwZnYfbkv/crrO8MZ5DW7PnrF4uRS4S1mOUmY2SNLrLX3OICg3pZL3diDr0/CteWeY2T5UibxXrSLJYB5qeVs8wNl8uEfpo3jQs69zvYBG1xuwp6QrzOwGfDe0f6W/wl9CN0lPF2rB0aj8BYBl8R7T34ChwExJxzUerprvybwPsD0+/zsFD5Nwl6THzawbPqf7gz2jzWxhSbPS57GSdk5lDcDN/NbHezd34g5P/YBfprp0BMZIeirzezT3mwVBpShG3tuTrEt60cymUS3y3hbDnpYe+NBtDnAUsFBW+lrAoXiPI2eEzRxl9aDBQmMEcC5uJncjySGohXXLKN9h+GY4mfTtgL/g4aEHZefNynM6PtxcGOiDD0F/j++tsGsT9zwW7109DIwHFs861xX3/F0VGJfj2lE0WK2EpVYcVXeUSt7bgax3Bgamz1Ul7xVvRE38oAvjYZXfSz/qrlnnCvYExc3tTsI9TzMB2+bDez234ruttaReGTPA/YGDs+tECgDXxLUvkuVclZU+OjW6jvlePr4vwxH40PZz3JlpjazzewNPAxfhMYqyG2AokDiq+iiFvLcXWU95qkreq35qC74PeHY43hN4AfilkuldAdduBayH7xT4Ob695ZPAU5K+NLNukr5qymoiT7k3Aj8GTsZ7Bp80k38x3FP3IHmcnowz6LeSZGaPAAdIerbRdUPxHtujkh5PaavhQei2wgVl2fR3GL5bWjd8bnUWcKOkDwp9riCoNK2V9zqX9S1xGR8haVJan6kaea8JRZLB3Gt7A+B5SR+28Nr+eCP7Mb4ANwe3T7+qFfXohNt3b4Sb2y1Eiigq6V858mfMBw/Hh+iHNDrfF3hY0go5rr0V+CdwuXwxspN8v2bS4uGKeGiE9fBe1xw8XPTGuOPVr1VLLzkIEq2V93qU9XT+MHzh/Q2qTN5rSpG0lKyX2gE3K/xWvnfz4rhd+f/kC3Mt3dSmM9476IH3BpbATRS/lnRaE9ctD1yBhzG4HV+wWxzfZ+Al+aLd94uAZtYLeAzfFe67RmWtjXvLXoabIv4SmB+3bb8T3zfhO3kYiFhgD+qadiDrK6ZyPqUK5b1uFUlWw1oY+FNKng8fKt8PPCdpTivK7YrH11kPD4q4I74/QQfc/vvL5l6kmW2Oz+OOwOc5b8MX8z7KvtbMtgD2kzQyWXl8nXWuO+4MtQU+jO8ErIn3wlbH7d53VyvMpYOglmhPsp6UY0eqTN6rPmhjEXTAAxYeiA8H5+A216/hNtfjgeMKLSxrXnVv/Hc7Hrch/yL1GFaXdDn8MMBaozI6pvP/MLPJkg4xs57Zc66Nrn0K+NrMhkqalMroLOkbvGF+QEMk0Y6S7gEeM998Z81QIkE7od5lfRPgg6REeuMKpFM1yXuHSt243KjBRjwz/bM6Hlbgt8Bk/MVlNosppLzMcHMNfC+BH+PxbMD/qa9XQHlKB8C5Zvbj1DhyXiPpXdwT93YzO9l8Q51vzMMi7J/ufxuwC3CsmU01swtwL94HCnmuIKh12oGsH5B1/1upQnmvW0UC3w8Jbwa+wG2zP0+nlsbj3kDDyy6Ucfh00q54Twe8cV2fpw5LZhqPpO/SItoSwBBJ/86k57uZpPPwhtMfuNXMXgROwB2tJuEjkb0kbZjyfQ5cb2aDWvhcQVCztANZv97MVsTlfe9qk/e6XyPJ+j4cX/CaDkyTNKoVC28r4EPo3fAFr4m4+d1bkvbOc83xuN34OsD7kh4295AdIekWM+si6X/5ngF8CGwe0bMjbjnyvjzkw3a4VcuZwJyshbsWe+0GQa1S57I+C5ghj2ZctfJet4oEwMw2wa0t5uJhFr7ArS5mS/rQCrQnN7MlJb1hZjcD+0qaY2Y/wxe4nsAjeM5tXF5qHItKmmlmx+B231/jwd7uL3ZO0zwMxBB8QfFGPBTER8oyGQyC9kC9y3q6x1h8uq3q5L3uFIk1BH/bHDgPb1QZi4238Dj+97WgvN64yd4oPN7NcEnTss6PBm7K19NIeVbFG/kcvCFkwkHPlLRXC58vY6HSRdL/Ui+oE7AhLjwPA2cVIjRBUMvUu6yn8r6Xd3x/kQm4d3xVyXvdKZIMZnYq7sh0s7nH6CrAasCrki5sYVk98HnRRfEG+h3+Qr8EfiFp8zzXrYR7116HW328mIat3XC7cCQ9UmhvKZVpeBTQPXGzwq/xPQ4+Tz2nFSRd0NKhfBDUKvUq66ncjLK8GFdMxwFPVJu817P571P4tM/NyZxuUjKT6wyFRcU0dxLaV9LZZrZ9mqcciC+4rY9vNnNpyvuDecrUiNbF4+SsAIw1s1clfQF8kXout8IPF+CyeiCLAT1Tg/y+rumc4Y5Jy+C26WeY2cv4fgkXZPIV9esFQe1Ql7LeiFXxEde2wMhqk/e6GpFYwxabu+DD0+G4VcM44GJJL7WwvE54b+Ib4CHgD3j4gjlZeZrsYZjZ2cBP8N5KX/wf/2u481HefRHMbAc8Yuk+8u0zM40u83dBPDTKQbh54t7AA5IOaMkzBkEtUs+yntK+l3fcO/6f6RlXpgrlvd7MfzMveUc8Zn9PfD+AhYCp6UUXjKS5kp6X9CLeI/gOeNHMHjDfVAZ8mmkezOwuM9tW0pGpEe2Lh57ukup0fcrXMc+9x+OWJ9ea2U6e1ZYHOprZHrht+ZWSnpR0Dr5P+4upzHp7r0HQmLqV9Yz8mtlKaaRxHr6PyRdVK+9q43DD5T5w7X0Lvmd5p0bn+qa/HYu8x3L4Xswj85zviEcKfR6Yjc+bDm2Ux7L/NnGvjfGG9Bs8Kijp3lPxXso+eGyhh4GNCikzjjjq4ahjWR+BB4nMyPs/8NHNXdUq73Uz10zQCgAACCFJREFUtZU1FNwAD5XQF/g7vqfyVEnTK1SvZfHpp1H4gt2dwNnAO2r042c9wwH4GshcvCGvDGyDB2bbQW6SuBi+2L4dPiS/Re7JGwR1TTuQ9c3xCMM3S/pdyl/V8l43iiRDsuCYgEfaXBnfK7krvs/xo21Yjw7A2kr7CqS0zJ4GN0v6S76FNTMbg29W8ww+PF4I74lMx5/tcuBNNWzP2YUCg8gFQb1Qx7L+InABHqjxr8B0Sa9lXVN18l4XVltZ2n0FPKDaiSm9O24GOAz/J9yWLA+MN99XeQJwqzxMwvBMhuwGYGnvAXNb9UGSxliDr8h2eKiGx4BD8CH9S2Y2BXeQejdXmUFQb9S7rKfz2wHvpuvnAB+a2Zv4vu9VKe91MSKxBlvrn+Nx+s8Bnpab31WiPtnhobcGdsZ7HW8Dv5F0f75rzOwfwGB8V7j/pHMT8C01n8DDZD+AOzl1wa1MHpV0W9kfLAgqTDuQ9a74lNiN+KL/zbiCrGp5r4sRiRpM8tYHeuNWExOTrfXbKnBb3hJiZjZQ0puS7gDuMLNV8Dg889i2ZzWsgfho40jcN2QEHu9nED6V9Q4eqA0z64M3sB8D7zcuMwjqkXqWdXnI+H/i2wNfqwZflUerXd7rZUSS/aJ6AJvilg/dgdeBU9JLaqv6rI8He3sGeAm3uPgS+D9gp3wv38x+CywjaT8z+zM+rL0aD8uwbpqLVb6Fu7I9UBBUCe1N1mHe6atqlPeaVyRZQ9018DnFlYCxkiaY2RA8hMANbfHjm1lfSe+bb++5HR40rgveM1kFn9/c1/I4NqVneEXSp2a2CHAKHlL6GUkn2bwetVXXoIKgXLRnWU/XVK2817wiATCz+fDwAdfgL3JX4ENgS0lftlEdhuE25Hfi1hYP4SZ9a+Dzmx/i8YDezW5cWcIxJNV/e0kvp3MH47F1NpT0UjU3pCBoC0LWq5PKe0QWgTV4dO6I9wDOlXS2pDXwl7tP/qtLziCgD754tgMequFU/DceJ2lCxtqiUQ8l4y07Eh+e72QNHrA3AgcphXuopYYVBKUkZL26qWlFkvWSPgY+NrPsEAaf4EPfNgkhIGkssDVutjcFd0T6CDgCeNB8v4Rc12WGr1vj4RQG4iGiwTewWRC+j/obBO2SkPXqpmatttJCG5I+k3S7me0MvGBm1+KLbjvjYUXakv+kv/vhexJcmP6uiy/G5ZznNLN1gTckPW1mDwAnmdn9+FD5GKi9HkoQlIqQ9eqnZtdIzGw3fEvLybhd9TQz2xjYA++h3CZpYhvVZR18XnQu7mU7FzdLnI3vufxtrgW3rOuXwdvPK+n7KcBOeLiHbfMt2AVBeyBkvfqp2REJ8AG+8czqwDAzmwk8AuyptPVkG1lv9MY9zj8A/ow3sq3xacOPgEUkzWji+s74ntDPmNnneIO8CA9HfWkmW9keIAiqn5D1KqdmRyTw/Xxob3w4uSIwP77Z07PARZK+aoM6dMQ3mzkUj/fzB+AcSV8XeP1S6ZrOwDt4r+sRYHFJ/ypLpYOgxghZr25qUpGY2YrpYz+8Yb2GOwUtiVtSvCJpSAXq1QfYC/g1Hub9/yRdVkhvydzTdYt0/SL4Jjf3lrvOQVDNhKzXBjWnSMxsIWAGrtVPxr0/F8Bty/vhGv5DSbNzOfW0YT2XS/W7SdLtefIsgIc8+LekT7PS7wCOl/RcrdmTB0GpCFmvHWpRkXTETecOxe25TwDGS/qkkvVqDebhFQ7G50qn44LxFXChpI0qWLUgqDgh67VDzfmRSPpW0jhJ6+OLb/2AaWb2HzPbD6rfDtvMfpHmfF8CzgIm4Q5OJwBXAbenfDm35gyC9kDIeu1QcyOSfJjvTnYKTQwvq4FkA7+fpI2TffxC+MJbf3wI/xHwuXwfkpod6gZBuQhZrz7qRpHUCmZ2HR5G4U4zOwv4WmlzniAI6of2JOs1N7VVyyQ79N1wyxOAoXjwtsyGNkEQ1AHtTdZjRNKGpHnQUcBRwKL48HbhSlmbBEFQHtqbrIciqRCpxzIaj68zHbimUDv0IAhqh/Yg66FIqoBaWTwMgqA46lXWQ5EEQRAERRGL7UEQBEFRhCIJgiAIiiIUSRAEQVAUoUhagZkdZ2ZTzOw5M3vGzNY1s8PMbP4Cri0oXxAE1UHIe/PEYnsLMbP1gHOAjSR9bWZ9gS7/394dszgRRlEYfk+jIiJiI2stCFaCCoKVjbC9CFtpteVWghY2+hcsBCs7EcHWQsUVFC1EQbGxEK1sFkV0O+FazBcJIbiTHTQR3qec3IRMksudySTfoYvePF5VG1vc/2OfOknzZ7/34xnJ7JaAjVGYTfuAnAUOAo9b/jJJbiR52Y5krrZta1PqziR5nuRVkrttTR5Ji8F+78Ezkhm1N/4pXULbQ7rfgz+ZPPJIsr+qvrR/uD4C1lrmwO+6dnRzD1iuqs0kl4CdVXVtDrsmaYL93s//nNk+F1X1I8kxupCa08CdJJenlJ5Lskr3Gi8BR4A3EzUn2/ZnbTXsHXShPZIWgP3ej4NkG9p6OevAepK3wPnx29NlM18ETlTV1yS3gF1THirAg6pa+bvPWNJ22e9b8xrJjJIcbsscjBwFPgHf6WJAAfYCm8C3JAeA5bH68boXwKkkh9pj704X2ylpAdjv/XhGMrs9wPUk+4CfdIuwrQIrwP0kn1uQzWvgHfCBLlZz5OZE3QXg9tjS0leA9/9oXyT9mf3egxfbJUmD+NWWJGkQB4kkaRAHiSRpEAeJJGkQB4kkaRAHiSRpEAeJJGmQXxWwqfyFtzh7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2)   #,constrained_layout=True\n",
    "\n",
    "sns.catplot(x=\"State\", y=\"Test AUC\",kind=\"point\", data=Results_df,color='r',ax=axs[0])\n",
    "axs[0].set_xticklabels(labels=axs[0].get_xticklabels(),rotation=70,size=10)\n",
    "axs[0].set(ylim=(0, 100))\n",
    "\n",
    "axs[1].set(ylim=(-200, 4700))\n",
    "\n",
    "sns.barplot(x=\"State\", y=\"# Features\", data=Results_df,ax=axs[1])\n",
    "axs[1].set_xticklabels(labels=axs[1].get_xticklabels(),rotation=70,size=10)\n",
    "fig.subplots_adjust(wspace=2)\n",
    "\n",
    "plt.close(2)\n",
    "plt.close(3)\n",
    "\n",
    "# plt.savefig('C:/Users/MASNA.CO/Dropbox/PAPERS/10-/Results/2Round_TPM_LR.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EIN3dT2e3c9"
   },
   "source": [
    "# ------------End of Pipeline------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T09:01:18.817110Z",
     "start_time": "2021-03-08T08:57:01.514Z"
    }
   },
   "outputs": [],
   "source": [
    "## Do not run this cell :))\n",
    "# os.system(\"shutdown /h\") #hibernate\n",
    "# normalization_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T09:01:18.818111Z",
     "start_time": "2021-03-08T08:57:01.518Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalization_selected = \"TMM\" # \"TPM\" \"TMM\"  \"ruvseq_diff\"    \"ruvseq_cv\"\n",
    "# normalisation_method_chosen=normalization_selected\n",
    "# Normalize (X_train,X_test,y_train,1,normalization_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T09:01:18.819111Z",
     "start_time": "2021-03-08T08:57:01.520Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train = ensg_id_to_genes_name(X_train).transpose()\n",
    "# normalisation_method_chosen = \"ruvseq_diff\" #  \"TMM\" \"TPM\" \"ruvseq_diff\"    \"ruvseq_cv\"\n",
    "# normalisation(X_train, 0, \"training\", data_id=\"GSE89843\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:34:29.786431Z",
     "start_time": "2021-03-13T07:34:29.780428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T13:35:21.006042Z",
     "start_time": "2021-03-08T13:35:20.673406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'learner': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=75, n_jobs=1,\n",
       "                       oob_score=False, random_state=1, verbose=False,\n",
       "                       warm_start=False), 'preprocs': (), 'ex_preprocs': ()},\n",
       " {'learner': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                         max_depth=4, max_features=0.13048713929631472,\n",
       "                         max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                         min_impurity_split=None, min_samples_leaf=13,\n",
       "                         min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                         n_estimators=32, n_jobs=1, oob_score=False,\n",
       "                         random_state=0, verbose=False, warm_start=False),\n",
       "  'preprocs': (),\n",
       "  'ex_preprocs': ()},\n",
       " {'learner': ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=4, max_features=0.22198274294609716,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=15,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=63, n_jobs=1, oob_score=False, random_state=2,\n",
       "                       verbose=False, warm_start=False),\n",
       "  'preprocs': (),\n",
       "  'ex_preprocs': ()},\n",
       " {'learner': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=14, n_jobs=1,\n",
       "                       oob_score=False, random_state=4, verbose=False,\n",
       "                       warm_start=False), 'preprocs': (), 'ex_preprocs': ()},\n",
       " {'learner': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=0.9363877287176361,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=23, n_jobs=1, oob_score=False, random_state=0,\n",
       "                       verbose=False, warm_start=False),\n",
       "  'preprocs': (),\n",
       "  'ex_preprocs': ()}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Best_model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Test1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "320px",
    "width": "261px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "382px",
    "left": "85px",
    "top": "110px",
    "width": "290.98px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
